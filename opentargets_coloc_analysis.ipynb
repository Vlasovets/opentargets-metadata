{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6306a0",
   "metadata": {},
   "source": [
    "### OpenTargets Genetics Data Processing & Analysis Workflow\n",
    "\n",
    "This notebook provides a reproducible workflow for batch processing, filtering, and analyzing OpenTargets Genetics datasets. It includes robust functions for:\n",
    "\n",
    "- **Memory-efficient batch filtering** of large and small OpenTargets datasets using manifest-driven column selection.\n",
    "- **Colocalization analysis**: Extracts high-confidence GWAS-QTL colocalizations, dynamically selecting available columns and providing comprehensive summary statistics.\n",
    "- **Gene-drug-target mapping**: Integrates genetic loci, gene annotations, drug information, and disease associations to generate actionable gene-drug-target tables.\n",
    "\n",
    "Each function is designed to handle missing columns gracefully, optimize memory usage, and output detailed diagnostics. The analyses support downstream interpretation of genetic evidence, variant-level colocalization, and drug target prioritization. This workflow is suitable for large-scale genetics projects and can be adapted to evolving OpenTargets data releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4440584f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in ./miniconda3/envs/jupyter_env/lib/python3.10/site-packages (21.0.0)\n",
      "Requirement already satisfied: duckdb in ./miniconda3/envs/jupyter_env/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: pandas==2.2.2 in ./miniconda3/envs/jupyter_env/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy==2.0.2 in ./miniconda3/envs/jupyter_env/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./miniconda3/envs/jupyter_env/lib/python3.10/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/envs/jupyter_env/lib/python3.10/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./miniconda3/envs/jupyter_env/lib/python3.10/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/envs/jupyter_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pyarrow\n",
    "!pip install duckdb\n",
    "!pip install --upgrade pandas==2.2.2 numpy==2.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbcb4d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import duckdb\n",
    "import time\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3cd67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_dataset(dataset_name, columns, base_path, output_path):\n",
    "    \"\"\"\n",
    "    Process a single Open Targets dataset by selecting specified columns and \n",
    "    writing a cleaned Parquet file to the output directory with memory-efficient handling.\n",
    "\n",
    "    This function:\n",
    "    - Checks if the dataset directory exists\n",
    "    - Reads the dataset schema to validate requested columns\n",
    "    - Streams large datasets (e.g., 'colocalisation_coloc', 'credible_set') in batches\n",
    "      to prevent memory overflow\n",
    "    - Saves a filtered version of the dataset with only valid columns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name : str\n",
    "        Name of the dataset (e.g., 'study', 'l2g_prediction') to be processed.\n",
    "    columns : list of str\n",
    "        List of column names to extract from the dataset.\n",
    "    base_path : str or Path\n",
    "        Base directory path containing subdirectories for each dataset.\n",
    "    output_path : str or Path\n",
    "        Directory where the processed Parquet files will be saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if processing was successful and the output file was written;\n",
    "        False otherwise (e.g., directory missing, no valid columns, or error encountered).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - For very large datasets, batch processing is used to minimize memory usage.\n",
    "    - Invalid columns (not present in schema) are skipped with a warning.\n",
    "    - Requires `pyarrow.dataset` and `pyarrow.parquet`.\n",
    "    - Performs garbage collection periodically to release memory.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> process_single_dataset(\"study\", [\"studyId\", \"geneId\"], base_path, output_path)\n",
    "    âœ… Saved study: 5,000 rows, 2 columns (1.2s)\n",
    "    True\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ”„ Processing {dataset_name}...\")\n",
    "    dataset_dir = os.path.join(base_path, dataset_name)\n",
    "    \n",
    "    try:\n",
    "        # Check if directory exists\n",
    "        if not os.path.exists(dataset_dir):\n",
    "            print(f\"âš ï¸  Directory does not exist: {dataset_dir}\")\n",
    "            return False\n",
    "            \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Only load valid parquet files\n",
    "        dataset = ds.dataset(\n",
    "            dataset_dir, \n",
    "            format=\"parquet\", \n",
    "            exclude_invalid_files=True\n",
    "        )\n",
    "        \n",
    "        # Get schema to check available columns\n",
    "        schema = dataset.schema\n",
    "        available_columns = [field.name for field in schema]\n",
    "        print(f\"ğŸ“‹ Available columns ({len(available_columns)}): {available_columns[:10]}{'...' if len(available_columns) > 10 else ''}\")\n",
    "        \n",
    "        # Filter columns to only those that exist\n",
    "        valid_columns = [col for col in columns if col in available_columns]\n",
    "        missing_columns = [col for col in columns if col not in available_columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"âš ï¸  Missing columns: {missing_columns}\")\n",
    "        \n",
    "        if valid_columns:\n",
    "            print(f\"ğŸ”„ Converting to table with {len(valid_columns)} columns: {valid_columns}\")\n",
    "            \n",
    "            # Process in chunks for large datasets to avoid memory issues\n",
    "            if dataset_name in ['colocalisation_coloc', 'credible_set']:\n",
    "                print(f\"ğŸ“Š Large dataset detected, using batch processing...\")\n",
    "                \n",
    "                # For very large datasets, process in batches\n",
    "                batch_size = 1000000  # 1M rows at a time\n",
    "                batches = dataset.to_batches(columns=valid_columns, batch_size=batch_size)\n",
    "                \n",
    "                output_file = os.path.join(output_path, f\"{dataset_name}.parquet\")\n",
    "                \n",
    "                # Write first batch to create the file\n",
    "                first_batch = next(batches)\n",
    "                # Convert batch directly to table (no pandas conversion needed)\n",
    "                table = pa.Table.from_batches([first_batch])\n",
    "                \n",
    "                parquet_writer = pq.ParquetWriter(output_file, table.schema)\n",
    "                parquet_writer.write_table(table)\n",
    "                \n",
    "                total_rows = len(table)\n",
    "                batch_count = 1\n",
    "                \n",
    "                # Process remaining batches\n",
    "                for batch in batches:\n",
    "                    # Convert batch directly to table\n",
    "                    batch_table = pa.Table.from_batches([batch])\n",
    "                    parquet_writer.write_table(batch_table)\n",
    "                    total_rows += len(batch_table)\n",
    "                    batch_count += 1\n",
    "                    \n",
    "                    if batch_count % 10 == 0:\n",
    "                        print(f\"  ğŸ“Š Processed {batch_count} batches, {total_rows:,} rows so far...\")\n",
    "                        gc.collect()  # Force garbage collection\n",
    "                \n",
    "                parquet_writer.close()\n",
    "                \n",
    "            else:\n",
    "                # For smaller datasets, process normally\n",
    "                table = dataset.to_table(columns=valid_columns)\n",
    "                output_file = os.path.join(output_path, f\"{dataset_name}.parquet\")\n",
    "                pq.write_table(table, output_file)\n",
    "                total_rows = len(table)\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"âœ… Saved {dataset_name}: {total_rows:,} rows, {len(valid_columns)} columns ({elapsed:.1f}s)\")\n",
    "            \n",
    "            # Force cleanup\n",
    "            del dataset\n",
    "            if 'table' in locals():\n",
    "                del table\n",
    "            gc.collect()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        else:\n",
    "            print(f\"âŒ No valid columns found for {dataset_name}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to process {dataset_name}: {e}\")\n",
    "        # Force cleanup on error\n",
    "        gc.collect()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aa92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gwas_colocalizations_with_available_columns(output_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze GWAS-QTL colocalizations with dynamic column selection and comprehensive statistics.\n",
    "    \n",
    "    This function performs a robust analysis of genetic colocalization data by first checking\n",
    "    what columns are available in the filtered OpenTargets datasets, then building a dynamic\n",
    "    SQL query to extract maximum information. It focuses on high-confidence colocalizations\n",
    "    (H4 > 0.8) and provides detailed statistical summaries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    output_path : str\n",
    "        Path to directory containing filtered OpenTargets parquet files. Expected files:\n",
    "        - colocalisation_coloc.parquet : Colocalization evidence between study pairs\n",
    "        - credible_set.parquet : Fine-mapped credible sets for study loci  \n",
    "        - study.parquet : Study metadata including traits and types\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame or None\n",
    "        DataFrame containing colocalization analysis with available columns, or None if failed.\n",
    "        \n",
    "        Core columns (always present):\n",
    "        - leftStudyLocusId : Study-locus identifier for GWAS signal\n",
    "        - rightStudyLocusId : Study-locus identifier for QTL signal\n",
    "        - studyId : Study identifier from credible set\n",
    "        - chromosome : Chromosomal location of colocalization\n",
    "        \n",
    "        Optional columns (included if available in source data):\n",
    "        - numberColocalisingVariants : Number of variants supporting colocalization\n",
    "        - variantId : Lead variant identifier (rs ID or chr:pos format)\n",
    "        - isTransQtl : Boolean indicating trans-QTL (distant) vs cis-QTL (local)\n",
    "        - h0, h1, h2, h3, h4 : Posterior probabilities for colocalization hypotheses\n",
    "        - traitFromSource : Human-readable trait description\n",
    "        - studyType : Type of left-side study (gwas, eqtl, pqtl, etc.)\n",
    "        - rightStudyType : Type of right-side study (eqtl, pqtl, etc.)\n",
    "        \n",
    "    Algorithm\n",
    "    ---------\n",
    "    1. **Schema Detection**: Checks available columns in each parquet file\n",
    "    2. **Dynamic Query Building**: Constructs SQL with only available columns\n",
    "    3. **High-Confidence Filtering**: Selects colocalizations with H4 > 0.8\n",
    "    4. **Multi-table Join**: Links colocalization â†’ credible set â†’ study metadata\n",
    "    5. **Statistical Analysis**: Computes comprehensive summaries by column type\n",
    "    \n",
    "    Query Logic\n",
    "    -----------\n",
    "    ```sql\n",
    "    SELECT [dynamic_columns]\n",
    "    FROM colocalisation_coloc (H4 > 0.8, sample 10K rows)\n",
    "    LEFT JOIN credible_set ON rightStudyLocusId = studyLocusId  \n",
    "    LEFT JOIN study ON credible_set.studyId = study.studyId\n",
    "    WHERE studyId IS NOT NULL\n",
    "    ORDER BY H4 DESC LIMIT 100\n",
    "    ```\n",
    "    \n",
    "    Statistical Outputs\n",
    "    ------------------\n",
    "    - **Data Completeness**: Missing value analysis for each column\n",
    "    - **Variant Analysis**: Statistics on numberColocalisingVariants (mean, median, range)\n",
    "    - **Variant ID Analysis**: Uniqueness and example variant identifiers\n",
    "    - **Trans-QTL Analysis**: Proportion of trans vs cis regulatory effects\n",
    "    - **Colocalization Evidence**: Mean posterior probabilities (H0-H4)\n",
    "    - **Study Type Distribution**: Breakdown by GWAS, eQTL, pQTL types\n",
    "    - **Cross-tabulations**: Variants by study type combinations\n",
    "    \n",
    "    Colocalization Hypotheses (H-values)\n",
    "    ------------------------------------\n",
    "    - H0: Neither trait has genetic association at locus\n",
    "    - H1: Only left trait (typically GWAS) has association  \n",
    "    - H2: Only right trait (typically QTL) has association\n",
    "    - H3: Both traits associated, but different causal variants\n",
    "    - H4: Both traits associated, shared causal variant (colocalization)\n",
    "    \n",
    "    Memory Management\n",
    "    ----------------\n",
    "    - Uses DuckDB for efficient parquet processing\n",
    "    - Samples large datasets (10K rows) to prevent memory issues\n",
    "    - Automatic connection cleanup via context manager\n",
    "    \n",
    "    Error Handling\n",
    "    --------------\n",
    "    - Graceful handling of missing parquet files\n",
    "    - Column availability checking prevents SQL errors\n",
    "    - Detailed error reporting with query debugging information\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Basic usage\n",
    "    >>> df = analyze_gwas_colocalizations_with_available_columns('/path/to/filtered_data')\n",
    "    >>> print(f\"Found {len(df)} high-confidence colocalizations\")\n",
    "    \n",
    "    >>> # Check for specific analyses\n",
    "    >>> if 'isTransQtl' in df.columns:\n",
    "    >>>     trans_qtl_pct = df['isTransQtl'].mean() * 100\n",
    "    >>>     print(f\"Trans-QTL percentage: {trans_qtl_pct:.1f}%\")\n",
    "    \n",
    "    >>> # Filter by study type\n",
    "    >>> eqtl_colocs = df[df['rightStudyType'] == 'eqtl']\n",
    "    >>> print(f\"eQTL colocalizations: {len(eqtl_colocs)}\")\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - Function prints extensive diagnostic information during execution\n",
    "    - Designed to work with any subset of OpenTargets columns\n",
    "    - H4 > 0.8 threshold represents high-confidence shared causal variants\n",
    "    - Sample size (10K rows) balances comprehensiveness with performance\n",
    "    - Results ordered by H4 (highest confidence first)\n",
    "    \n",
    "    See Also\n",
    "    --------\n",
    "    - OpenTargets Genetics documentation: https://genetics-docs.opentargets.org/\n",
    "    - Colocalization methods: Giambartolomei et al. (2014) PLoS Genet\n",
    "    - COLOC R package: https://github.com/chr1swallace/coloc\n",
    "    \"\"\"\n",
    "    \n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    print(\"ğŸ” Checking what columns are actually available in your filtered datasets...\")\n",
    "    \n",
    "    # Check available columns in each table\n",
    "    tables_info = {}\n",
    "    \n",
    "    for table_name in ['colocalisation_coloc', 'credible_set', 'study']:\n",
    "        try:\n",
    "            query = f\"SELECT * FROM read_parquet('{output_path}/{table_name}.parquet') LIMIT 1\"\n",
    "            sample = con.execute(query).df()\n",
    "            tables_info[table_name] = sample.columns.tolist()\n",
    "            print(f\"  ğŸ“‹ {table_name}: {sample.columns.tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {table_name}: Error - {e}\")\n",
    "            tables_info[table_name] = []\n",
    "    \n",
    "    # Build query with only available columns\n",
    "    print(f\"\\nğŸ”§ Building query with available columns...\")\n",
    "    \n",
    "    # Core columns that should always be there\n",
    "    select_columns = [\n",
    "        \"coloc.leftStudyLocusId\",\n",
    "        \"coloc.rightStudyLocusId\",\n",
    "        \"credible.studyId\", \n",
    "        \"coloc.chromosome\"\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Add h-values if available\n",
    "    h_columns = ['h0', 'h1', 'h2', 'h3', 'h4']\n",
    "    available_h_cols = []\n",
    "    for h_col in h_columns:\n",
    "        if h_col in tables_info.get('colocalisation_coloc', []):\n",
    "            select_columns.append(f\"coloc.{h_col}\")\n",
    "            available_h_cols.append(h_col)\n",
    "    \n",
    "    print(f\"  âœ… Available h-columns: {available_h_cols}\")\n",
    "    \n",
    "    # Add study columns if available\n",
    "    study_cols = ['traitFromSource', 'studyType']\n",
    "    available_study_cols = []\n",
    "    for col in study_cols:\n",
    "        if col in tables_info.get('study', []):\n",
    "            select_columns.append(f\"study.{col}\")\n",
    "            available_study_cols.append(col)\n",
    "    \n",
    "    print(f\"  âœ… Available study columns: {available_study_cols}\")\n",
    "\n",
    "    # Add numberColocalisingVariants if available\n",
    "    if 'numberColocalisingVariants' in tables_info.get('colocalisation_coloc', []):\n",
    "        select_columns.append(\"coloc.numberColocalisingVariants\")\n",
    "        print(f\"  âœ… numberColocalisingVariants: Available\")\n",
    "    else:\n",
    "        print(f\"  âŒ numberColocalisingVariants: Not available in your filtered dataset\")\n",
    "    \n",
    "    # Add variantId if available\n",
    "    if 'variantId' in tables_info.get('credible_set', []):\n",
    "        select_columns.append(\"credible.variantId\")\n",
    "        print(f\"  âœ… variantId: Available\")\n",
    "    else:\n",
    "        print(f\"  âŒ variantId: Not available in your filtered dataset\")\n",
    "\n",
    "    # Add isTransQtl if available\n",
    "    if 'isTransQtl' in tables_info.get('credible_set', []):\n",
    "        select_columns.append(\"credible.isTransQtl\")\n",
    "        print(f\"  âœ… isTransQtl: Available\")\n",
    "    else:\n",
    "        print(f\"  âŒ isTransQtl: Not available in your filtered dataset\")\n",
    "    \n",
    "    # Check for rightStudyType specifically\n",
    "    if 'rightStudyType' in tables_info.get('colocalisation_coloc', []):\n",
    "        select_columns.append(\"coloc.rightStudyType\")\n",
    "        print(f\"  âœ… rightStudyType: Available\")\n",
    "    else:\n",
    "        print(f\"  âŒ rightStudyType: Not available in your filtered dataset\")\n",
    "    \n",
    "    # Build the final query\n",
    "    select_clause = \",\\n      \".join(select_columns)\n",
    "    \n",
    "    specific_query = f\"\"\"\n",
    "    SELECT \n",
    "    {select_clause}                                    -- Dynamic column selection based on available data\n",
    "    FROM (\n",
    "    -- Step 1: Filter high-confidence colocalizations from main dataset\n",
    "    SELECT * FROM read_parquet('{output_path}/colocalisation_coloc.parquet')\n",
    "    WHERE h4 > 0.8                                     -- Only include strong colocalization evidence (>80% probability)\n",
    "    USING SAMPLE 10000 ROWS                            -- Random sample to prevent memory issues with large datasets\n",
    "    ) coloc\n",
    "    -- Step 2: Join with credible sets to get variant-level information\n",
    "    LEFT JOIN read_parquet('{output_path}/credible_set.parquet') credible\n",
    "    ON coloc.rightStudyLocusId = credible.studyLocusId  -- Link colocalization to fine-mapped variants\n",
    "    -- Step 3: Join with study metadata to get trait and publication information  \n",
    "    LEFT JOIN read_parquet('{output_path}/study.parquet') study\n",
    "    ON credible.studyId = study.studyId                  -- Link credible sets to study descriptions\n",
    "    WHERE credible.studyId IS NOT NULL                   -- Filter out colocalizations without credible set matches\n",
    "    ORDER BY coloc.h4 DESC                               -- Sort by colocalization confidence (highest first)\n",
    "    LIMIT 100                                            -- Return top 100 results for manageable analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸš€ Executing query with {len(select_columns)} available columns...\")\n",
    "    print(f\"ğŸ“‹ Selected columns: {[col.split('.')[-1] for col in select_columns]}\")\n",
    "    \n",
    "    try:\n",
    "        df_specific = con.execute(specific_query).df()\n",
    "        print(f\"âœ… Dataset created: {len(df_specific)} rows\")\n",
    "        print(f\"ğŸ“‹ Final columns: {df_specific.columns.tolist()}\")\n",
    "        \n",
    "        # Data completeness analysis\n",
    "        print(f\"\\nğŸ“Š Data completeness:\")\n",
    "        for col in df_specific.columns:\n",
    "            non_null = df_specific[col].notna().sum()\n",
    "            print(f\"  {col}: {non_null}/{len(df_specific)} ({non_null/len(df_specific)*100:.1f}%)\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nğŸ‘€ Sample of your dataset:\")\n",
    "        print(df_specific.head())\n",
    "        \n",
    "        # Analyze numberColocalisingVariants if available\n",
    "        if 'numberColocalisingVariants' in df_specific.columns:\n",
    "            print(f\"\\nğŸ”— Colocalising Variants Analysis:\")\n",
    "            variants_stats = df_specific['numberColocalisingVariants'].describe()\n",
    "            print(f\"  Mean variants per colocalization: {variants_stats['mean']:.1f}\")\n",
    "            print(f\"  Median variants: {variants_stats['50%']:.1f}\")\n",
    "            print(f\"  Range: {variants_stats['min']:.0f} - {variants_stats['max']:.0f}\")\n",
    "            print(f\"  High variant count (>10): {(df_specific['numberColocalisingVariants'] > 10).sum()} pairs\")\n",
    "        \n",
    "        # Analyze variantId if available\n",
    "        if 'variantId' in df_specific.columns:\n",
    "            print(f\"\\nğŸ§¬ Variant ID Analysis:\")\n",
    "            unique_variants = df_specific['variantId'].nunique()\n",
    "            total_rows = len(df_specific)\n",
    "            print(f\"  Unique variants: {unique_variants:,}\")\n",
    "            print(f\"  Total rows: {total_rows:,}\")\n",
    "            print(f\"  Variants per row ratio: {unique_variants/total_rows:.2f}\")\n",
    "            \n",
    "            # Show some example variant IDs\n",
    "            sample_variants = df_specific['variantId'].dropna().head().tolist()\n",
    "            print(f\"  Example variant IDs: {sample_variants}\")\n",
    "        \n",
    "        # Analyze isTransQtl if available\n",
    "        if 'isTransQtl' in df_specific.columns:\n",
    "            print(f\"\\nğŸ§¬ Trans-QTL Analysis:\")\n",
    "            trans_qtl_count = df_specific['isTransQtl'].sum()\n",
    "            total_rows = len(df_specific)\n",
    "            print(f\"  Trans-QTL colocalizations: {trans_qtl_count}/{total_rows} ({trans_qtl_count/total_rows*100:.1f}%)\")\n",
    "            print(f\"  Cis-QTL colocalizations: {total_rows - trans_qtl_count}/{total_rows} ({(total_rows - trans_qtl_count)/total_rows*100:.1f}%)\")\n",
    "        \n",
    "        # Analyze colocalization evidence if h-values available\n",
    "        if available_h_cols:\n",
    "            print(f\"\\nğŸ§¬ Colocalization analysis:\")\n",
    "            print(f\"  Available evidence: {available_h_cols}\")\n",
    "            print(f\"  Mean posterior probabilities:\")\n",
    "            for h_col in available_h_cols:\n",
    "                if h_col in df_specific.columns:\n",
    "                    mean_val = df_specific[h_col].mean()\n",
    "                    print(f\"    {h_col}: {mean_val:.4f}\")\n",
    "        \n",
    "        # Study type analysis if available\n",
    "        if 'studyType' in df_specific.columns:\n",
    "            print(f\"\\nğŸ“Š Study type distribution:\")\n",
    "            study_counts = df_specific['studyType'].value_counts()\n",
    "            for study_type, count in study_counts.items():\n",
    "                print(f\"  {study_type}: {count} rows\")\n",
    "        \n",
    "        # Right study type analysis if available\n",
    "        if 'rightStudyType' in df_specific.columns:\n",
    "            print(f\"\\nğŸ“Š Right study type distribution:\")\n",
    "            right_study_counts = df_specific['rightStudyType'].value_counts()\n",
    "            for study_type, count in right_study_counts.items():\n",
    "                print(f\"  {study_type}: {count} rows\")\n",
    "        \n",
    "        # Cross-analysis if both variants and study types are available\n",
    "        if 'numberColocalisingVariants' in df_specific.columns and 'rightStudyType' in df_specific.columns:\n",
    "            print(f\"\\nğŸ”— Variants by right study type:\")\n",
    "            variants_by_type = df_specific.groupby('rightStudyType')['numberColocalisingVariants'].agg(['mean', 'count'])\n",
    "            for study_type, stats in variants_by_type.iterrows():\n",
    "                print(f\"  {study_type}: {stats['mean']:.1f} avg variants (n={stats['count']})\")\n",
    "        \n",
    "        return df_specific\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Query failed: {e}\")\n",
    "        print(f\"\\nQuery was:\")\n",
    "        print(specific_query)\n",
    "        return None\n",
    "    finally:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "834d1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gene_drug_target_dataframe(output_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a dataframe linking genes to drug targets using OpenTargets data.\n",
    "    \n",
    "    This function combines multiple datasets to create comprehensive gene-drug-target mappings:\n",
    "    - l2g_prediction: Links genetic loci to genes (geneId)\n",
    "    - evidence: Links targets to diseases (targetId) \n",
    "    - known_drug: Links drugs to targets (drugId, targetId)\n",
    "    - target: Provides gene symbols and annotations\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    output_path : str\n",
    "        Path to directory containing filtered OpenTargets parquet files\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with gene and drug target information\n",
    "    \"\"\"\n",
    "    \n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    print(\"ğŸ” Checking available datasets for gene-drug-target mapping...\")\n",
    "    \n",
    "    # Check available columns in each relevant table\n",
    "    tables_info = {}\n",
    "    required_tables = ['l2g_prediction', 'evidence', 'known_drug', 'target']\n",
    "    \n",
    "    for table_name in required_tables:\n",
    "        try:\n",
    "            query = f\"SELECT * FROM read_parquet('{output_path}/{table_name}.parquet') LIMIT 1\"\n",
    "            sample = con.execute(query).df()\n",
    "            tables_info[table_name] = sample.columns.tolist()\n",
    "            print(f\"  ğŸ“‹ {table_name}: {sample.columns.tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {table_name}: Error - {e}\")\n",
    "            tables_info[table_name] = []\n",
    "    \n",
    "    # Build dynamic query based on available columns\n",
    "    print(f\"\\nğŸ”§ Building gene-drug-target query...\")\n",
    "    \n",
    "    # Core columns we want to extract\n",
    "    select_columns = []\n",
    "    \n",
    "    # From l2g_prediction: geneId, studyLocusId, score\n",
    "    if 'geneId' in tables_info.get('l2g_prediction', []):\n",
    "        select_columns.extend([\n",
    "            \"l2g.geneId\",\n",
    "            \"l2g.studyLocusId\", \n",
    "            \"l2g.score as l2g_score\"\n",
    "        ])\n",
    "        print(f\"  âœ… l2g_prediction: geneId available\")\n",
    "    \n",
    "    # From evidence: targetId, diseaseId\n",
    "    if 'targetId' in tables_info.get('evidence', []):\n",
    "        select_columns.extend([\n",
    "            \"evidence.targetId\",\n",
    "            \"evidence.diseaseId\"\n",
    "        ])\n",
    "        print(f\"  âœ… evidence: targetId available\")\n",
    "    \n",
    "    # From known_drug: drugId and available columns (check what actually exists)\n",
    "    known_drug_cols = []\n",
    "    if 'drugId' in tables_info.get('known_drug', []):\n",
    "        known_drug_cols.append(\"drug.drugId\")\n",
    "        \n",
    "        # Check for optional columns that might exist\n",
    "        if 'phase' in tables_info.get('known_drug', []):\n",
    "            known_drug_cols.append(\"drug.phase\")\n",
    "        if 'status' in tables_info.get('known_drug', []):\n",
    "            known_drug_cols.append(\"drug.status\")\n",
    "            \n",
    "        select_columns.extend(known_drug_cols)\n",
    "        print(f\"  âœ… known_drug: {[col.split('.')[-1] for col in known_drug_cols]} available\")\n",
    "    \n",
    "    # From target: gene symbol and annotations\n",
    "    target_cols = []\n",
    "    if 'approvedSymbol' in tables_info.get('target', []):\n",
    "        target_cols.append(\"target.approvedSymbol as gene_symbol\")\n",
    "    if 'biotype' in tables_info.get('target', []):\n",
    "        target_cols.append(\"target.biotype\")\n",
    "    \n",
    "    if target_cols:\n",
    "        select_columns.extend(target_cols)\n",
    "        print(f\"  âœ… target: {[col.split('.')[-1].split(' as ')[-1] for col in target_cols]} available\")\n",
    "    \n",
    "    if not select_columns:\n",
    "        print(\"âŒ No required columns found in datasets\")\n",
    "        return None\n",
    "    \n",
    "    # Build the comprehensive query\n",
    "    select_clause = \",\\n      \".join(select_columns)\n",
    "    \n",
    "    gene_drug_query = f\"\"\"\n",
    "    SELECT \n",
    "      {select_clause}\n",
    "    FROM (\n",
    "      -- Start with locus-to-gene predictions (high confidence genes)\n",
    "      SELECT * FROM read_parquet('{output_path}/l2g_prediction.parquet')\n",
    "      WHERE score > 0.5  -- Only include high-confidence gene predictions\n",
    "      USING SAMPLE 50000 ROWS  -- Sample to manage memory\n",
    "    ) l2g\n",
    "    -- Join with evidence to get target-disease associations\n",
    "    LEFT JOIN read_parquet('{output_path}/evidence.parquet') evidence\n",
    "      ON l2g.geneId = evidence.targetId  -- Gene ID = Target ID in OpenTargets\n",
    "    -- Join with known drugs to get drug information\n",
    "    LEFT JOIN read_parquet('{output_path}/known_drug.parquet') drug\n",
    "      ON evidence.targetId = drug.targetId \n",
    "      AND evidence.diseaseId = drug.diseaseId  -- Match target-disease pairs\n",
    "    -- Join with target info to get gene symbols\n",
    "    LEFT JOIN read_parquet('{output_path}/target.parquet') target\n",
    "      ON l2g.geneId = target.id  -- Gene ID matches target ID\n",
    "    WHERE evidence.targetId IS NOT NULL  -- Ensure we have target information\n",
    "      AND drug.drugId IS NOT NULL        -- Ensure we have drug information\n",
    "    ORDER BY l2g.score DESC  -- Prioritize high-confidence genes\n",
    "    LIMIT 1000  -- Return top 1000 gene-drug-target associations\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸš€ Executing gene-drug-target query...\")\n",
    "    print(f\"ğŸ“‹ Selected columns: {[col.split('.')[-1].split(' as ')[-1] for col in select_columns]}\")\n",
    "    \n",
    "    try:\n",
    "        df_gene_drug = con.execute(gene_drug_query).df()\n",
    "        print(f\"âœ… Dataset created: {len(df_gene_drug)} rows\")\n",
    "        print(f\"ğŸ“‹ Final columns: {df_gene_drug.columns.tolist()}\")\n",
    "        \n",
    "        # Data summary\n",
    "        print(f\"\\nğŸ“Š Gene-Drug-Target Summary:\")\n",
    "        if 'geneId' in df_gene_drug.columns:\n",
    "            unique_genes = df_gene_drug['geneId'].nunique()\n",
    "            print(f\"  Unique genes: {unique_genes:,}\")\n",
    "        \n",
    "        if 'drugId' in df_gene_drug.columns:\n",
    "            unique_drugs = df_gene_drug['drugId'].nunique()\n",
    "            print(f\"  Unique drugs: {unique_drugs:,}\")\n",
    "        \n",
    "        if 'targetId' in df_gene_drug.columns:\n",
    "            unique_targets = df_gene_drug['targetId'].nunique()\n",
    "            print(f\"  Unique targets: {unique_targets:,}\")\n",
    "        \n",
    "        # Clinical phase analysis (only if phase column exists)\n",
    "        if 'phase' in df_gene_drug.columns:\n",
    "            print(f\"\\nğŸ’Š Drug Development Phases:\")\n",
    "            phase_counts = df_gene_drug['phase'].value_counts().sort_index()\n",
    "            for phase, count in phase_counts.items():\n",
    "                phase_name = {0: \"Preclinical\", 1: \"Phase I\", 2: \"Phase II\", \n",
    "                            3: \"Phase III\", 4: \"Approved\"}.get(phase, f\"Phase {phase}\")\n",
    "                print(f\"  {phase_name}: {count} associations\")\n",
    "        \n",
    "        # L2G score analysis\n",
    "        if 'l2g_score' in df_gene_drug.columns:\n",
    "            print(f\"\\nğŸ¯ Locus-to-Gene Score Distribution:\")\n",
    "            score_stats = df_gene_drug['l2g_score'].describe()\n",
    "            print(f\"  Mean score: {score_stats['mean']:.3f}\")\n",
    "            print(f\"  High confidence (>0.8): {(df_gene_drug['l2g_score'] > 0.8).sum()} genes\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nğŸ‘€ Sample gene-drug-target associations:\")\n",
    "        display_cols = ['geneId', 'gene_symbol', 'drugId', 'targetId', 'l2g_score']\n",
    "        if 'phase' in df_gene_drug.columns:\n",
    "            display_cols.insert(-1, 'phase')\n",
    "        available_display_cols = [col for col in display_cols if col in df_gene_drug.columns]\n",
    "        print(df_gene_drug[available_display_cols].head())\n",
    "        \n",
    "        return df_gene_drug\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Query failed: {e}\")\n",
    "        print(f\"\\nQuery was:\")\n",
    "        print(gene_drug_query)\n",
    "        return None\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "# Usage example - make sure to define the paths\n",
    "def run_gene_drug_analysis():\n",
    "    \"\"\"Run the gene-drug-target analysis and save results.\"\"\"\n",
    "    \n",
    "    print(\"ğŸš€ Creating gene-drug-target dataframe...\")\n",
    "    df_gene_drug = create_gene_drug_target_dataframe(output_path)\n",
    "    \n",
    "    if df_gene_drug is not None:\n",
    "        print(f\"\\nğŸ‰ Analysis complete!\")\n",
    "        print(f\"ğŸ“Š Generated {len(df_gene_drug)} gene-drug-target associations\")\n",
    "        \n",
    "        # Save the dataset\n",
    "        output_file = main_dir / \"gene_drug_target_associations.csv\"\n",
    "        df_gene_drug.to_csv(output_file, index=False)\n",
    "        print(f\"ğŸ’¾ Saved to: {output_file}\")\n",
    "        \n",
    "        # Additional analysis\n",
    "        print(f\"\\nğŸ”¬ Key Insights:\")\n",
    "        if 'gene_symbol' in df_gene_drug.columns and 'drugId' in df_gene_drug.columns:\n",
    "            # Top genes by drug count\n",
    "            top_genes = df_gene_drug.groupby('gene_symbol')['drugId'].nunique().sort_values(ascending=False).head()\n",
    "            print(f\"  Top genes by drug count:\")\n",
    "            for gene, drug_count in top_genes.items():\n",
    "                print(f\"    {gene}: {drug_count} drugs\")\n",
    "        \n",
    "        return df_gene_drug\n",
    "    else:\n",
    "        print(\"âŒ Failed to create gene-drug-target dataframe\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfcd58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = Path(\"/lustre/groups/itg/shared/referenceData/OpenTargets/\")\n",
    "\n",
    "base_path = main_dir / \"data_version_29_07\"\n",
    "output_path = main_dir / \"temp/01_filtered_parquets\"\n",
    "manifest_path = main_dir / \"temp/columns_manifest.json\"\n",
    "\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the columns manifest to get the list of datasets and their columns\n",
    "with manifest_path.open() as f:\n",
    "    manifest = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52906ae",
   "metadata": {},
   "source": [
    "### Batch Processing of OpenTargets Datasets with Memory-Efficient Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04a398eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Process 8 datasets\n",
      "\n",
      "============================================================\n",
      "Processing colocalisation_coloc (1/8)\n",
      "============================================================\n",
      "\n",
      "ğŸ”„ Processing colocalisation_coloc...\n",
      "ğŸ“‹ Available columns (12): ['leftStudyLocusId', 'rightStudyLocusId', 'chromosome', 'rightStudyType', 'numberColocalisingVariants', 'h0', 'h1', 'h2', 'h3', 'h4']...\n",
      "ğŸ”„ Converting to table with 10 columns: ['leftStudyLocusId', 'rightStudyLocusId', 'chromosome', 'h4', 'rightStudyType', 'numberColocalisingVariants', 'h0', 'h1', 'h2', 'h3']\n",
      "ğŸ“Š Large dataset detected, using batch processing...\n",
      "  ğŸ“Š Processed 10 batches, 5,626,147 rows so far...\n",
      "  ğŸ“Š Processed 20 batches, 10,630,414 rows so far...\n",
      "  ğŸ“Š Processed 30 batches, 15,424,575 rows so far...\n",
      "  ğŸ“Š Processed 40 batches, 21,052,744 rows so far...\n",
      "  ğŸ“Š Processed 50 batches, 26,057,870 rows so far...\n",
      "  ğŸ“Š Processed 60 batches, 30,844,974 rows so far...\n",
      "  ğŸ“Š Processed 70 batches, 36,474,946 rows so far...\n",
      "âœ… Saved colocalisation_coloc: 38,561,709 rows, 10 columns (43.3s)\n",
      "Memory cleanup completed.\n",
      "\n",
      "============================================================\n",
      "Processing credible_set (2/8)\n",
      "============================================================\n",
      "\n",
      "ğŸ”„ Processing credible_set...\n",
      "ğŸ“‹ Available columns (27): ['studyLocusId', 'studyId', 'variantId', 'chromosome', 'position', 'region', 'beta', 'zScore', 'pValueMantissa', 'pValueExponent']...\n",
      "ğŸ”„ Converting to table with 4 columns: ['studyLocusId', 'studyId', 'variantId', 'isTransQtl']\n",
      "ğŸ“Š Large dataset detected, using batch processing...\n",
      "  ğŸ“Š Processed 10 batches, 1,130,069 rows so far...\n",
      "  ğŸ“Š Processed 20 batches, 2,129,374 rows so far...\n",
      "âœ… Saved credible_set: 2,833,758 rows, 4 columns (3.1s)\n",
      "Memory cleanup completed.\n",
      "\n",
      "============================================================\n",
      "Processing l2g_prediction (3/8)\n",
      "============================================================\n",
      "\n",
      "ğŸ”„ Processing l2g_prediction...\n",
      "ğŸ“‹ Available columns (5): ['studyLocusId', 'geneId', 'score', 'features', 'shapBaseValue']\n",
      "ğŸ”„ Converting to table with 3 columns: ['studyLocusId', 'geneId', 'score']\n",
      "âœ… Saved l2g_prediction: 1,473,532 rows, 3 columns (2.0s)\n",
      "Memory cleanup completed.\n",
      "\n",
      "============================================================\n",
      "Processing evidence (4/8)\n",
      "============================================================\n",
      "\n",
      "ğŸ”„ Processing evidence...\n",
      "ğŸ“‹ Available columns (88): ['datasourceId', 'targetId', 'alleleOrigins', 'allelicRequirements', 'ancestry', 'ancestryId', 'beta', 'betaConfidenceIntervalLower', 'betaConfidenceIntervalUpper', 'biologicalModelAllelicComposition']...\n",
      "ğŸ”„ Converting to table with 3 columns: ['studyId', 'targetId', 'diseaseId']\n",
      "âœ… Saved evidence: 29,602,753 rows, 3 columns (88.9s)\n",
      "Memory cleanup completed.\n",
      "\n",
      "============================================================\n",
      "Processing known_drug (5/8)\n",
      "============================================================\n",
      "\n",
      "ğŸ”„ Processing known_drug...\n",
      "ğŸ“‹ Available columns (17): ['drugId', 'targetId', 'diseaseId', 'phase', 'status', 'urls', 'ancestors', 'label', 'approvedSymbol', 'approvedName']...\n",
      "ğŸ”„ Converting to table with 3 columns: ['targetId', 'diseaseId', 'drugId']\n",
      "âœ… Saved known_drug: 253,442 rows, 3 columns (0.2s)\n",
      "Memory cleanup completed.\n",
      "\n",
      "============================================================\n",
      "Processing disease (6/8)\n",
      "============================================================\n",
      "\n",
      "ğŸ”„ Processing disease...\n",
      "ğŸ“‹ Available columns (14): ['id', 'code', 'name', 'description', 'dbXRefs', 'parents', 'synonyms', 'obsoleteTerms', 'obsoleteXRefs', 'children']...\n",
      "ğŸ”„ Converting to table with 3 columns: ['id', 'name', 'description']\n",
      "âœ… Saved disease: 38,959 rows, 3 columns (0.4s)\n",
      "Memory cleanup completed.\n",
      "\n",
      "============================================================\n",
      "Processing study (7/8)\n",
      "============================================================\n",
      "\n",
      "ğŸ”„ Processing study...\n",
      "ğŸ“‹ Available columns (30): ['studyId', 'geneId', 'projectId', 'studyType', 'traitFromSource', 'traitFromSourceMappedIds', 'biosampleFromSourceId', 'pubmedId', 'publicationTitle', 'publicationFirstAuthor']...\n",
      "ğŸ”„ Converting to table with 5 columns: ['studyId', 'studyType', 'traitFromSource', 'projectId', 'pubmedId']\n",
      "âœ… Saved study: 1,966,178 rows, 5 columns (1.5s)\n",
      "Memory cleanup completed.\n",
      "\n",
      "============================================================\n",
      "Processing target (8/8)\n",
      "============================================================\n",
      "\n",
      "ğŸ”„ Processing target...\n",
      "ğŸ“‹ Available columns (29): ['id', 'approvedSymbol', 'biotype', 'transcriptIds', 'canonicalTranscript', 'canonicalExons', 'genomicLocation', 'alternativeGenes', 'approvedName', 'go']...\n",
      "ğŸ”„ Converting to table with 3 columns: ['id', 'approvedSymbol', 'biotype']\n",
      "âœ… Saved target: 78,726 rows, 3 columns (0.6s)\n",
      "Memory cleanup completed.\n",
      "\n",
      "ğŸ‰ Processing complete!\n",
      "âœ… Successfully processed: 8/8 datasets\n",
      "ğŸ“‚ Filtered datasets saved to: /lustre/groups/itg/shared/referenceData/OpenTargets/temp/01_filtered_parquets\n"
     ]
    }
   ],
   "source": [
    "print(f\"ğŸ“‹ Process {len(manifest)} datasets\")\n",
    "\n",
    "# Process datasets one by one to avoid memory issues\n",
    "success_count = 0\n",
    "failed_datasets = []\n",
    "\n",
    "# Process large datasets first (they're most likely to cause memory issues)\n",
    "large_datasets = ['colocalisation_coloc', 'credible_set']\n",
    "small_datasets = [name for name in manifest.keys() if name not in large_datasets]\n",
    "\n",
    "# Order: large datasets first, then small ones\n",
    "processing_order = large_datasets + small_datasets\n",
    "\n",
    "for dataset_name in processing_order:\n",
    "    if dataset_name in manifest:\n",
    "        columns = manifest[dataset_name]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing {dataset_name} ({success_count + 1}/{len(manifest)})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        success = process_single_dataset(dataset_name, columns, base_path, output_path)\n",
    "        \n",
    "        if success:\n",
    "            success_count += 1\n",
    "        else:\n",
    "            failed_datasets.append(dataset_name)\n",
    "        \n",
    "        # Force garbage collection between datasets\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Memory cleanup completed.\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Processing complete!\")\n",
    "print(f\"âœ… Successfully processed: {success_count}/{len(manifest)} datasets\")\n",
    "\n",
    "if failed_datasets:\n",
    "    print(f\"âŒ Failed datasets: {failed_datasets}\")\n",
    "\n",
    "print(f\"ğŸ“‚ Filtered datasets saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3881d025",
   "metadata": {},
   "source": [
    "### Colocalization Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c9b092d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Creating dataset with available columns including numberColocalisingVariants, variantId, and isTransQtl...\n",
      "ğŸ” Checking what columns are actually available in your filtered datasets...\n",
      "  ğŸ“‹ colocalisation_coloc: ['leftStudyLocusId', 'rightStudyLocusId', 'chromosome', 'h4', 'rightStudyType', 'numberColocalisingVariants', 'h0', 'h1', 'h2', 'h3']\n",
      "  ğŸ“‹ credible_set: ['studyLocusId', 'studyId', 'variantId', 'isTransQtl']\n",
      "  ğŸ“‹ study: ['studyId', 'studyType', 'traitFromSource', 'projectId', 'pubmedId']\n",
      "\n",
      "ğŸ”§ Building query with available columns...\n",
      "  âœ… Available h-columns: ['h0', 'h1', 'h2', 'h3', 'h4']\n",
      "  âœ… Available study columns: ['traitFromSource', 'studyType']\n",
      "  âœ… numberColocalisingVariants: Available\n",
      "  âœ… variantId: Available\n",
      "  âœ… isTransQtl: Available\n",
      "  âœ… rightStudyType: Available\n",
      "\n",
      "ğŸš€ Executing query with 15 available columns...\n",
      "ğŸ“‹ Selected columns: ['leftStudyLocusId', 'rightStudyLocusId', 'studyId', 'chromosome', 'h0', 'h1', 'h2', 'h3', 'h4', 'traitFromSource', 'studyType', 'numberColocalisingVariants', 'variantId', 'isTransQtl', 'rightStudyType']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81ffc48ab51494ab7afb05741177c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset created: 100 rows\n",
      "ğŸ“‹ Final columns: ['leftStudyLocusId', 'rightStudyLocusId', 'studyId', 'chromosome', 'h0', 'h1', 'h2', 'h3', 'h4', 'traitFromSource', 'studyType', 'numberColocalisingVariants', 'variantId', 'isTransQtl', 'rightStudyType']\n",
      "\n",
      "ğŸ“Š Data completeness:\n",
      "  leftStudyLocusId: 100/100 (100.0%)\n",
      "  rightStudyLocusId: 100/100 (100.0%)\n",
      "  studyId: 100/100 (100.0%)\n",
      "  chromosome: 100/100 (100.0%)\n",
      "  h0: 100/100 (100.0%)\n",
      "  h1: 100/100 (100.0%)\n",
      "  h2: 100/100 (100.0%)\n",
      "  h3: 100/100 (100.0%)\n",
      "  h4: 100/100 (100.0%)\n",
      "  traitFromSource: 100/100 (100.0%)\n",
      "  studyType: 100/100 (100.0%)\n",
      "  numberColocalisingVariants: 100/100 (100.0%)\n",
      "  variantId: 100/100 (100.0%)\n",
      "  isTransQtl: 11/100 (11.0%)\n",
      "  rightStudyType: 100/100 (100.0%)\n",
      "\n",
      "ğŸ‘€ Sample of your dataset:\n",
      "                   leftStudyLocusId                 rightStudyLocusId  \\\n",
      "0  8c3672c9822525cc91ec0e3b164ec7ec  363d740259e5ef96863c6690bf99f42e   \n",
      "1  b27d089888c12f39398df55581513e3f  48597ac988b6a51483baee8405eb3028   \n",
      "2  e792964bda81a15aae6c989ff142e946  873e6f2a9ca732268784845917413c97   \n",
      "3  c342e99fe3535b6a7275c1fe56b0aab9  70b7b538dc50f4915586687f3938ff12   \n",
      "4  83bd609f3e6f64d82119189a2157f400  6974810b53d25e2811af09ab322045b0   \n",
      "\n",
      "                                             studyId chromosome  \\\n",
      "0                                       GCST90092966          2   \n",
      "1                                       GCST90446053         11   \n",
      "2                                       GCST90092998         19   \n",
      "3  gtex_leafcutter_pituitary_16_72057466_72058254...         16   \n",
      "4                                       GCST90092966         11   \n",
      "\n",
      "              h0             h1             h2             h3   h4  \\\n",
      "0  4.103600e-250  6.717448e-153  6.108868e-101  4.103600e-258  1.0   \n",
      "1   0.000000e+00   0.000000e+00   7.930471e-28   0.000000e+00  1.0   \n",
      "2   0.000000e+00   0.000000e+00   0.000000e+00   0.000000e+00  1.0   \n",
      "3   1.285942e-46   6.479517e-28   1.984627e-22   1.285942e-54  1.0   \n",
      "4   0.000000e+00  4.061046e-265  5.333662e-122   0.000000e+00  1.0   \n",
      "\n",
      "                                     traitFromSource studyType  \\\n",
      "0                   Triglyceride levels in small LDL      gwas   \n",
      "1  Free Cholesterol to Cholesterol in Small VLDL ...      gwas   \n",
      "2                    Free cholesterol levels in VLDL      gwas   \n",
      "3                   16:72057466:72058254:clu_26386_+      sqtl   \n",
      "4                   Triglyceride levels in small LDL      gwas   \n",
      "\n",
      "   numberColocalisingVariants         variantId  isTransQtl rightStudyType  \n",
      "0                           1    2_27508073_T_C        <NA>           gwas  \n",
      "1                           1  11_117204850_C_T        <NA>           gwas  \n",
      "2                           1   19_44730118_A_G        <NA>           gwas  \n",
      "3                           1   16_72110275_T_C       False           sqtl  \n",
      "4                           1  11_116778201_G_C        <NA>           gwas  \n",
      "\n",
      "ğŸ”— Colocalising Variants Analysis:\n",
      "  Mean variants per colocalization: 1.0\n",
      "  Median variants: 1.0\n",
      "  Range: 1 - 1\n",
      "  High variant count (>10): 0 pairs\n",
      "\n",
      "ğŸ§¬ Variant ID Analysis:\n",
      "  Unique variants: 62\n",
      "  Total rows: 100\n",
      "  Variants per row ratio: 0.62\n",
      "  Example variant IDs: ['2_27508073_T_C', '11_117204850_C_T', '19_44730118_A_G', '16_72110275_T_C', '11_116778201_G_C']\n",
      "\n",
      "ğŸ§¬ Trans-QTL Analysis:\n",
      "  Trans-QTL colocalizations: 9/100 (9.0%)\n",
      "  Cis-QTL colocalizations: 91/100 (91.0%)\n",
      "\n",
      "ğŸ§¬ Colocalization analysis:\n",
      "  Available evidence: ['h0', 'h1', 'h2', 'h3', 'h4']\n",
      "  Mean posterior probabilities:\n",
      "    h0: 0.0000\n",
      "    h1: 0.0000\n",
      "    h2: 0.0000\n",
      "    h3: 0.0000\n",
      "    h4: 1.0000\n",
      "\n",
      "ğŸ“Š Study type distribution:\n",
      "  gwas: 89 rows\n",
      "  pqtl: 9 rows\n",
      "  sqtl: 1 rows\n",
      "  tuqtl: 1 rows\n",
      "\n",
      "ğŸ“Š Right study type distribution:\n",
      "  gwas: 89 rows\n",
      "  pqtl: 9 rows\n",
      "  sqtl: 1 rows\n",
      "  tuqtl: 1 rows\n",
      "\n",
      "ğŸ”— Variants by right study type:\n",
      "  gwas: 1.0 avg variants (n=89.0)\n",
      "  pqtl: 1.0 avg variants (n=9.0)\n",
      "  sqtl: 1.0 avg variants (n=1.0)\n",
      "  tuqtl: 1.0 avg variants (n=1.0)\n",
      "\n",
      "ğŸ‰ Analysis complete!\n",
      "ğŸ“Š Generated 100 rows with available columns\n",
      "ğŸ’¾ Saved to: /lustre/groups/itg/shared/referenceData/OpenTargets/colocalisation_available_columns_with_variants.csv\n",
      "\n",
      "ğŸ“‹ Comparison - Requested vs Available:\n",
      "  âœ… leftStudyLocusId\n",
      "  âœ… rightStudyLocusId\n",
      "  âœ… studyId\n",
      "  âœ… traitFromSource\n",
      "  âœ… chromosome\n",
      "  âœ… studyType\n",
      "  âœ… rightStudyType\n",
      "  âœ… numberColocalisingVariants\n",
      "  âœ… variantId\n",
      "  âœ… isTransQtl\n",
      "  âœ… h0\n",
      "  âœ… h1\n",
      "  âœ… h2\n",
      "  âœ… h3\n",
      "  âœ… h4\n",
      "\n",
      "ğŸ“Š Summary:\n",
      "  âœ… Available columns: 15/15 (100.0%)\n",
      "  ğŸ“‹ Available: ['leftStudyLocusId', 'rightStudyLocusId', 'studyId', 'traitFromSource', 'chromosome', 'studyType', 'rightStudyType', 'numberColocalisingVariants', 'variantId', 'isTransQtl', 'h0', 'h1', 'h2', 'h3', 'h4']\n",
      "\n",
      "ğŸ’¡ To get missing columns:\n",
      "  â€¢ Add isTransQtl to your manifest's credible_set columns\n",
      "  â€¢ Re-run the dataset filtering process\n",
      "  â€¢ Then run this analysis to get the complete dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ Creating dataset with available columns including numberColocalisingVariants, variantId, and isTransQtl...\")\n",
    "df_final_available = analyze_gwas_colocalizations_with_available_columns(output_path)\n",
    "\n",
    "if df_final_available is not None:\n",
    "    print(f\"\\nğŸ‰ Analysis complete!\")\n",
    "    print(f\"ğŸ“Š Generated {len(df_final_available)} rows with available columns\")\n",
    "    \n",
    "    # Save the dataset\n",
    "    available_file = main_dir / \"colocalisation_available_columns_with_variants.csv\"\n",
    "    df_final_available.to_csv(available_file, index=False)\n",
    "    print(f\"ğŸ’¾ Saved to: {available_file}\")\n",
    "    \n",
    "    # Show what you got vs what you wanted\n",
    "    print(f\"\\nğŸ“‹ Comparison - Requested vs Available:\")\n",
    "    requested = ['leftStudyLocusId','rightStudyLocusId', 'studyId', 'traitFromSource', 'chromosome', \n",
    "                'studyType', 'rightStudyType', 'numberColocalisingVariants', 'variantId',\n",
    "                'isTransQtl', 'h0', 'h1', 'h2', 'h3', 'h4']\n",
    "    \n",
    "    available_cols = []\n",
    "    missing_cols = []\n",
    "    for col in requested:\n",
    "        if col in df_final_available.columns:\n",
    "            available_cols.append(col)\n",
    "            print(f\"  âœ… {col}\")\n",
    "        else:\n",
    "            missing_cols.append(col)\n",
    "            print(f\"  âŒ {col} (not in your filtered dataset)\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Summary:\")\n",
    "    print(f\"  âœ… Available columns: {len(available_cols)}/{len(requested)} ({len(available_cols)/len(requested)*100:.1f}%)\")\n",
    "    print(f\"  ğŸ“‹ Available: {available_cols}\")\n",
    "    if missing_cols:\n",
    "        print(f\"  âŒ Missing: {missing_cols}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ To get missing columns:\")\n",
    "    print(f\"  â€¢ Add isTransQtl to your manifest's credible_set columns\")\n",
    "    print(f\"  â€¢ Re-run the dataset filtering process\")\n",
    "    print(f\"  â€¢ Then run this analysis to get the complete dataset\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Failed to create dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a8c4db",
   "metadata": {},
   "source": [
    "### Gene-Drug-Target Association Analysis and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2aed4bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Creating gene-drug-target dataframe...\n",
      "ğŸ” Checking available datasets for gene-drug-target mapping...\n",
      "  ğŸ“‹ l2g_prediction: ['studyLocusId', 'geneId', 'score']\n",
      "  ğŸ“‹ evidence: ['studyId', 'targetId', 'diseaseId']\n",
      "  ğŸ“‹ known_drug: ['targetId', 'diseaseId', 'drugId']\n",
      "  ğŸ“‹ target: ['id', 'approvedSymbol', 'biotype']\n",
      "\n",
      "ğŸ”§ Building gene-drug-target query...\n",
      "  âœ… l2g_prediction: geneId available\n",
      "  âœ… evidence: targetId available\n",
      "  âœ… known_drug: ['drugId'] available\n",
      "  âœ… target: ['gene_symbol', 'biotype'] available\n",
      "\n",
      "ğŸš€ Executing gene-drug-target query...\n",
      "ğŸ“‹ Selected columns: ['geneId', 'studyLocusId', 'l2g_score', 'targetId', 'diseaseId', 'drugId', 'gene_symbol', 'biotype']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a0476a52634e659908f4178c4b6ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset created: 1000 rows\n",
      "ğŸ“‹ Final columns: ['geneId', 'studyLocusId', 'l2g_score', 'targetId', 'diseaseId', 'drugId', 'gene_symbol', 'biotype']\n",
      "\n",
      "ğŸ“Š Gene-Drug-Target Summary:\n",
      "  Unique genes: 1\n",
      "  Unique drugs: 6\n",
      "  Unique targets: 1\n",
      "\n",
      "ğŸ¯ Locus-to-Gene Score Distribution:\n",
      "  Mean score: 1.000\n",
      "  High confidence (>0.8): 1000 genes\n",
      "\n",
      "ğŸ‘€ Sample gene-drug-target associations:\n",
      "            geneId gene_symbol         drugId         targetId  l2g_score\n",
      "0  ENSG00000113578        FGF1  CHEMBL4298055  ENSG00000113578   0.999998\n",
      "1  ENSG00000113578        FGF1  CHEMBL4298055  ENSG00000113578   0.999998\n",
      "2  ENSG00000113578        FGF1   CHEMBL413376  ENSG00000113578   0.999998\n",
      "3  ENSG00000113578        FGF1   CHEMBL265502  ENSG00000113578   0.999998\n",
      "4  ENSG00000113578        FGF1   CHEMBL265502  ENSG00000113578   0.999998\n",
      "\n",
      "ğŸ‰ Analysis complete!\n",
      "ğŸ“Š Generated 1000 gene-drug-target associations\n",
      "ğŸ’¾ Saved to: /lustre/groups/itg/shared/referenceData/OpenTargets/gene_drug_target_associations.csv\n",
      "\n",
      "ğŸ”¬ Key Insights:\n",
      "  Top genes by drug count:\n",
      "    FGF1: 6 drugs\n",
      "\n",
      "ğŸ“Š Gene-Drug-Target Data Analysis:\n",
      "  Total associations: 1,000\n",
      "  Unique genes: 1\n",
      "  Unique drugs: 6\n",
      "  Unique targets: 1\n",
      "  Unique diseases: 30\n",
      "\n",
      "ğŸ” Top genes by drug count:\n",
      "    FGF1: 6 drugs\n",
      "\n",
      "ğŸ¯ L2G Score Statistics:\n",
      "count    1.000000e+03\n",
      "mean     9.999979e-01\n",
      "std      4.443114e-16\n",
      "min      9.999979e-01\n",
      "25%      9.999979e-01\n",
      "50%      9.999979e-01\n",
      "75%      9.999979e-01\n",
      "max      9.999979e-01\n",
      "Name: l2g_score, dtype: float64\n",
      "\n",
      "ğŸ§¬ Biotype Distribution:\n",
      "biotype\n",
      "protein_coding    1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ‘€ Sample associations:\n",
      "            geneId                      studyLocusId  l2g_score  \\\n",
      "0  ENSG00000113578  4932d76f08996772aafd8743a1b34984   0.999998   \n",
      "1  ENSG00000113578  4932d76f08996772aafd8743a1b34984   0.999998   \n",
      "2  ENSG00000113578  4932d76f08996772aafd8743a1b34984   0.999998   \n",
      "3  ENSG00000113578  4932d76f08996772aafd8743a1b34984   0.999998   \n",
      "4  ENSG00000113578  4932d76f08996772aafd8743a1b34984   0.999998   \n",
      "\n",
      "          targetId      diseaseId         drugId gene_symbol         biotype  \n",
      "0  ENSG00000113578    EFO_0001421  CHEMBL4298055        FGF1  protein_coding  \n",
      "1  ENSG00000113578    EFO_0000400  CHEMBL4298055        FGF1  protein_coding  \n",
      "2  ENSG00000113578    EFO_0003060   CHEMBL413376        FGF1  protein_coding  \n",
      "3  ENSG00000113578  MONDO_0008315   CHEMBL265502        FGF1  protein_coding  \n",
      "4  ENSG00000113578    EFO_0003060   CHEMBL265502        FGF1  protein_coding  \n"
     ]
    }
   ],
   "source": [
    "df_gene_target = run_gene_drug_analysis()\n",
    "\n",
    "if df_gene_target is not None:\n",
    "    print(\"\\nğŸ“Š Gene-Drug-Target Data Analysis:\")\n",
    "    print(f\"  Total associations: {len(df_gene_target):,}\")\n",
    "    print(f\"  Unique genes: {df_gene_target['geneId'].nunique():,}\")\n",
    "    print(f\"  Unique drugs: {df_gene_target['drugId'].nunique():,}\")\n",
    "    print(f\"  Unique targets: {df_gene_target['targetId'].nunique():,}\")\n",
    "    print(f\"  Unique diseases: {df_gene_target['diseaseId'].nunique():,}\")\n",
    "\n",
    "    # Top genes by drug count\n",
    "    top_genes = df_gene_target.groupby('gene_symbol')['drugId'].nunique().sort_values(ascending=False).head(5)\n",
    "    print(\"\\nğŸ” Top genes by drug count:\")\n",
    "    for gene, count in top_genes.items():\n",
    "        print(f\"    {gene}: {count} drugs\")\n",
    "\n",
    "    # L2G score distribution\n",
    "    print(\"\\nğŸ¯ L2G Score Statistics:\")\n",
    "    print(df_gene_target['l2g_score'].describe())\n",
    "\n",
    "    # Drug development phase analysis (if available)\n",
    "    if 'phase' in df_gene_target.columns:\n",
    "        print(\"\\nğŸ’Š Drug Development Phases:\")\n",
    "        print(df_gene_target['phase'].value_counts().sort_index())\n",
    "\n",
    "    # Biotype distribution\n",
    "    print(\"\\nğŸ§¬ Biotype Distribution:\")\n",
    "    print(df_gene_target['biotype'].value_counts())\n",
    "\n",
    "    # Show sample data\n",
    "    print(\"\\nğŸ‘€ Sample associations:\")\n",
    "    print(df_gene_target.head())\n",
    "else:\n",
    "    print(\"âŒ No gene-drug-target associations found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
