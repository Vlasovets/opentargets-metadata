{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6306a0",
   "metadata": {},
   "source": [
    "### OpenTargets Genetics Data Processing & Analysis Workflow\n",
    "\n",
    "This notebook provides a reproducible workflow for batch processing, filtering, and analyzing OpenTargets Genetics datasets. It includes robust functions for:\n",
    "\n",
    "- **Memory-efficient batch filtering** of large and small OpenTargets datasets using manifest-driven column selection.\n",
    "- **Colocalization analysis**: Extracts high-confidence GWAS-QTL colocalizations, dynamically selecting available columns and providing comprehensive summary statistics.\n",
    "- **Gene-drug-target mapping**: Integrates genetic loci, gene annotations, drug information, and disease associations to generate actionable gene-drug-target tables.\n",
    "\n",
    "Each function is designed to handle missing columns gracefully, optimize memory usage, and output detailed diagnostics. The analyses support downstream interpretation of genetic evidence, variant-level colocalization, and drug target prioritization. This workflow is suitable for large-scale genetics projects and can be adapted to evolving OpenTargets data releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4440584f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in ./miniconda3/envs/jupyter_env/lib/python3.10/site-packages (21.0.0)\n",
      "Requirement already satisfied: duckdb in ./miniconda3/envs/jupyter_env/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: pandas==2.2.2 in ./miniconda3/envs/jupyter_env/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy==2.0.2 in ./miniconda3/envs/jupyter_env/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./miniconda3/envs/jupyter_env/lib/python3.10/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/envs/jupyter_env/lib/python3.10/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./miniconda3/envs/jupyter_env/lib/python3.10/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/envs/jupyter_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pyarrow\n",
    "!pip install duckdb\n",
    "!pip install --upgrade pandas==2.2.2 numpy==2.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbcb4d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import duckdb\n",
    "import time\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0d3cd67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_dataset(dataset_name, columns, base_path, output_path):\n",
    "    \"\"\"\n",
    "    Process a single Open Targets dataset by selecting specified columns and \n",
    "    writing a cleaned Parquet file to the output directory with memory-efficient handling.\n",
    "\n",
    "    This function:\n",
    "    - Checks if the dataset directory exists\n",
    "    - Reads the dataset schema to validate requested columns\n",
    "    - Streams large datasets (e.g., 'colocalisation_coloc', 'credible_set') in batches\n",
    "      to prevent memory overflow\n",
    "    - Saves a filtered version of the dataset with only valid columns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name : str\n",
    "        Name of the dataset (e.g., 'study', 'l2g_prediction') to be processed.\n",
    "    columns : list of str\n",
    "        List of column names to extract from the dataset.\n",
    "    base_path : str or Path\n",
    "        Base directory path containing subdirectories for each dataset.\n",
    "    output_path : str or Path\n",
    "        Directory where the processed Parquet files will be saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if processing was successful and the output file was written;\n",
    "        False otherwise (e.g., directory missing, no valid columns, or error encountered).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - For very large datasets, batch processing is used to minimize memory usage.\n",
    "    - Invalid columns (not present in schema) are skipped with a warning.\n",
    "    - Requires `pyarrow.dataset` and `pyarrow.parquet`.\n",
    "    - Performs garbage collection periodically to release memory.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> process_single_dataset(\"study\", [\"studyId\", \"geneId\"], base_path, output_path)\n",
    "    âœ… Saved study: 5,000 rows, 2 columns (1.2s)\n",
    "    True\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nðŸ”„ Processing {dataset_name}...\")\n",
    "    dataset_dir = os.path.join(base_path, dataset_name)\n",
    "    \n",
    "    try:\n",
    "        # Check if directory exists\n",
    "        if not os.path.exists(dataset_dir):\n",
    "            print(f\"âš ï¸  Directory does not exist: {dataset_dir}\")\n",
    "            return False\n",
    "            \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Only load valid parquet files\n",
    "        dataset = ds.dataset(\n",
    "            dataset_dir, \n",
    "            format=\"parquet\", \n",
    "            exclude_invalid_files=True\n",
    "        )\n",
    "        \n",
    "        # Get schema to check available columns\n",
    "        schema = dataset.schema\n",
    "        available_columns = [field.name for field in schema]\n",
    "        print(f\"ðŸ“‹ Available columns ({len(available_columns)}): {available_columns[:10]}{'...' if len(available_columns) > 10 else ''}\")\n",
    "        \n",
    "        # Filter columns to only those that exist\n",
    "        valid_columns = [col for col in columns if col in available_columns]\n",
    "        missing_columns = [col for col in columns if col not in available_columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"âš ï¸  Missing columns: {missing_columns}\")\n",
    "        \n",
    "        if valid_columns:\n",
    "            print(f\"ðŸ”„ Converting to table with {len(valid_columns)} columns: {valid_columns}\")\n",
    "            \n",
    "            # Process in chunks for large datasets to avoid memory issues\n",
    "            if dataset_name in ['colocalisation_coloc', 'credible_set']:\n",
    "                print(f\"ðŸ“Š Large dataset detected, using batch processing...\")\n",
    "                \n",
    "                # For very large datasets, process in batches\n",
    "                batch_size = 1000000  # 1M rows at a time\n",
    "                batches = dataset.to_batches(columns=valid_columns, batch_size=batch_size)\n",
    "                \n",
    "                output_file = os.path.join(output_path, f\"{dataset_name}.parquet\")\n",
    "                \n",
    "                # Write first batch to create the file\n",
    "                first_batch = next(batches)\n",
    "                # Convert batch directly to table (no pandas conversion needed)\n",
    "                table = pa.Table.from_batches([first_batch])\n",
    "                \n",
    "                parquet_writer = pq.ParquetWriter(output_file, table.schema)\n",
    "                parquet_writer.write_table(table)\n",
    "                \n",
    "                total_rows = len(table)\n",
    "                batch_count = 1\n",
    "                \n",
    "                # Process remaining batches\n",
    "                for batch in batches:\n",
    "                    # Convert batch directly to table\n",
    "                    batch_table = pa.Table.from_batches([batch])\n",
    "                    parquet_writer.write_table(batch_table)\n",
    "                    total_rows += len(batch_table)\n",
    "                    batch_count += 1\n",
    "                    \n",
    "                    if batch_count % 10 == 0:\n",
    "                        print(f\"  ðŸ“Š Processed {batch_count} batches, {total_rows:,} rows so far...\")\n",
    "                        gc.collect()  # Force garbage collection\n",
    "                \n",
    "                parquet_writer.close()\n",
    "                \n",
    "            else:\n",
    "                # For smaller datasets, process normally\n",
    "                table = dataset.to_table(columns=valid_columns)\n",
    "                output_file = os.path.join(output_path, f\"{dataset_name}.parquet\")\n",
    "                pq.write_table(table, output_file)\n",
    "                total_rows = len(table)\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"âœ… Saved {dataset_name}: {total_rows:,} rows, {len(valid_columns)} columns ({elapsed:.1f}s)\")\n",
    "            \n",
    "            # Force cleanup\n",
    "            del dataset\n",
    "            if 'table' in locals():\n",
    "                del table\n",
    "            gc.collect()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        else:\n",
    "            print(f\"âŒ No valid columns found for {dataset_name}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to process {dataset_name}: {e}\")\n",
    "        # Force cleanup on error\n",
    "        gc.collect()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30aa92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gwas_colocalizations_with_available_columns(output_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze GWAS-QTL colocalizations with dynamic column selection and comprehensive statistics.\n",
    "    \n",
    "    This function performs a robust analysis of genetic colocalization data by first checking\n",
    "    what columns are available in the filtered OpenTargets datasets, then building a dynamic\n",
    "    SQL query to extract maximum information. It focuses on high-confidence colocalizations\n",
    "    (H4 > 0.8) and provides detailed statistical summaries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    output_path : str\n",
    "        Path to directory containing filtered OpenTargets parquet files. Expected files:\n",
    "        - colocalisation_coloc.parquet : Colocalization evidence between study pairs\n",
    "        - credible_set.parquet : Fine-mapped credible sets for study loci  \n",
    "        - study.parquet : Study metadata including traits and types\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame or None\n",
    "        DataFrame containing colocalization analysis with available columns, or None if failed.\n",
    "        \n",
    "        Core columns (always present):\n",
    "        - leftStudyLocusId : Study-locus identifier for GWAS signal\n",
    "        - rightStudyLocusId : Study-locus identifier for QTL signal\n",
    "        - studyId : Study identifier from credible set\n",
    "        - chromosome : Chromosomal location of colocalization\n",
    "        \n",
    "        Optional columns (included if available in source data):\n",
    "        - numberColocalisingVariants : Number of variants supporting colocalization\n",
    "        - variantId : Lead variant identifier (rs ID or chr:pos format)\n",
    "        - isTransQtl : Boolean indicating trans-QTL (distant) vs cis-QTL (local)\n",
    "        - h0, h1, h2, h3, h4 : Posterior probabilities for colocalization hypotheses\n",
    "        - traitFromSource : Human-readable trait description\n",
    "        - studyType : Type of left-side study (gwas, eqtl, pqtl, etc.)\n",
    "        - rightStudyType : Type of right-side study (eqtl, pqtl, etc.)\n",
    "        \n",
    "    Algorithm\n",
    "    ---------\n",
    "    1. **Schema Detection**: Checks available columns in each parquet file\n",
    "    2. **Dynamic Query Building**: Constructs SQL with only available columns\n",
    "    3. **High-Confidence Filtering**: Selects colocalizations with H4 > 0.8\n",
    "    4. **Multi-table Join**: Links colocalization â†’ credible set â†’ study metadata\n",
    "    5. **Statistical Analysis**: Computes comprehensive summaries by column type\n",
    "    \n",
    "    Query Logic\n",
    "    -----------\n",
    "    ```sql\n",
    "    SELECT [dynamic_columns]\n",
    "    FROM colocalisation_coloc (H4 > 0.8, sample 10K rows)\n",
    "    LEFT JOIN credible_set ON rightStudyLocusId = studyLocusId  \n",
    "    LEFT JOIN study ON credible_set.studyId = study.studyId\n",
    "    WHERE studyId IS NOT NULL\n",
    "    ORDER BY H4 DESC LIMIT 100\n",
    "    ```\n",
    "    \n",
    "    Statistical Outputs\n",
    "    ------------------\n",
    "    - **Data Completeness**: Missing value analysis for each column\n",
    "    - **Variant Analysis**: Statistics on numberColocalisingVariants (mean, median, range)\n",
    "    - **Variant ID Analysis**: Uniqueness and example variant identifiers\n",
    "    - **Trans-QTL Analysis**: Proportion of trans vs cis regulatory effects\n",
    "    - **Colocalization Evidence**: Mean posterior probabilities (H0-H4)\n",
    "    - **Study Type Distribution**: Breakdown by GWAS, eQTL, pQTL types\n",
    "    - **Cross-tabulations**: Variants by study type combinations\n",
    "    \n",
    "    Colocalization Hypotheses (H-values)\n",
    "    ------------------------------------\n",
    "    - H0: Neither trait has genetic association at locus\n",
    "    - H1: Only left trait (typically GWAS) has association  \n",
    "    - H2: Only right trait (typically QTL) has association\n",
    "    - H3: Both traits associated, but different causal variants\n",
    "    - H4: Both traits associated, shared causal variant (colocalization)\n",
    "    \n",
    "    Memory Management\n",
    "    ----------------\n",
    "    - Uses DuckDB for efficient parquet processing\n",
    "    - Samples large datasets (10K rows) to prevent memory issues\n",
    "    - Automatic connection cleanup via context manager\n",
    "    \n",
    "    Error Handling\n",
    "    --------------\n",
    "    - Graceful handling of missing parquet files\n",
    "    - Column availability checking prevents SQL errors\n",
    "    - Detailed error reporting with query debugging information\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Basic usage\n",
    "    >>> df = analyze_gwas_colocalizations_with_available_columns('/path/to/filtered_data')\n",
    "    >>> print(f\"Found {len(df)} high-confidence colocalizations\")\n",
    "    \n",
    "    >>> # Check for specific analyses\n",
    "    >>> if 'isTransQtl' in df.columns:\n",
    "    >>>     trans_qtl_pct = df['isTransQtl'].mean() * 100\n",
    "    >>>     print(f\"Trans-QTL percentage: {trans_qtl_pct:.1f}%\")\n",
    "    \n",
    "    >>> # Filter by study type\n",
    "    >>> eqtl_colocs = df[df['rightStudyType'] == 'eqtl']\n",
    "    >>> print(f\"eQTL colocalizations: {len(eqtl_colocs)}\")\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - Function prints extensive diagnostic information during execution\n",
    "    - Designed to work with any subset of OpenTargets columns\n",
    "    - H4 > 0.8 threshold represents high-confidence shared causal variants\n",
    "    - Sample size (10K rows) balances comprehensiveness with performance\n",
    "    - Results ordered by H4 (highest confidence first)\n",
    "    \n",
    "    See Also\n",
    "    --------\n",
    "    - OpenTargets Genetics documentation: https://genetics-docs.opentargets.org/\n",
    "    - Colocalization methods: Giambartolomei et al. (2014) PLoS Genet\n",
    "    - COLOC R package: https://github.com/chr1swallace/coloc\n",
    "    \"\"\"\n",
    "    \n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    print(\"ðŸ” Checking what columns are actually available in your filtered datasets...\")\n",
    "    \n",
    "    # Check available columns in each table\n",
    "    tables_info = {}\n",
    "    \n",
    "    for table_name in ['colocalisation_coloc', 'credible_set', 'study']:\n",
    "        try:\n",
    "            query = f\"SELECT * FROM read_parquet('{output_path}/{table_name}.parquet') LIMIT 1\"\n",
    "            sample = con.execute(query).df()\n",
    "            tables_info[table_name] = sample.columns.tolist()\n",
    "            print(f\"  ðŸ“‹ {table_name}: {sample.columns.tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {table_name}: Error - {e}\")\n",
    "            tables_info[table_name] = []\n",
    "    \n",
    "    # Build query with only available columns\n",
    "    print(f\"\\nðŸ”§ Building query with available columns...\")\n",
    "    \n",
    "    # Core columns that should always be there\n",
    "    select_columns = [\n",
    "        \"coloc.leftStudyLocusId\",\n",
    "        \"coloc.rightStudyLocusId\",\n",
    "        \"credible.studyId\", \n",
    "        \"coloc.chromosome\"\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Add h-values if available\n",
    "    h_columns = ['h0', 'h1', 'h2', 'h3', 'h4']\n",
    "    available_h_cols = []\n",
    "    for h_col in h_columns:\n",
    "        if h_col in tables_info.get('colocalisation_coloc', []):\n",
    "            select_columns.append(f\"coloc.{h_col}\")\n",
    "            available_h_cols.append(h_col)\n",
    "    \n",
    "    print(f\"  âœ… Available h-columns: {available_h_cols}\")\n",
    "    \n",
    "    # Add study columns if available\n",
    "    study_cols = ['traitFromSource', 'studyType']\n",
    "    available_study_cols = []\n",
    "    for col in study_cols:\n",
    "        if col in tables_info.get('study', []):\n",
    "            select_columns.append(f\"study.{col}\")\n",
    "            available_study_cols.append(col)\n",
    "    \n",
    "    print(f\"  âœ… Available study columns: {available_study_cols}\")\n",
    "\n",
    "    # Add numberColocalisingVariants if available\n",
    "    if 'numberColocalisingVariants' in tables_info.get('colocalisation_coloc', []):\n",
    "        select_columns.append(\"coloc.numberColocalisingVariants\")\n",
    "        print(f\"  âœ… numberColocalisingVariants: Available\")\n",
    "    else:\n",
    "        print(f\"  âŒ numberColocalisingVariants: Not available in your filtered dataset\")\n",
    "    \n",
    "    # Add variantId if available\n",
    "    if 'variantId' in tables_info.get('credible_set', []):\n",
    "        select_columns.append(\"credible.variantId\")\n",
    "        print(f\"  âœ… variantId: Available\")\n",
    "    else:\n",
    "        print(f\"  âŒ variantId: Not available in your filtered dataset\")\n",
    "\n",
    "    # Add isTransQtl if available\n",
    "    if 'isTransQtl' in tables_info.get('credible_set', []):\n",
    "        select_columns.append(\"credible.isTransQtl\")\n",
    "        print(f\"  âœ… isTransQtl: Available\")\n",
    "    else:\n",
    "        print(f\"  âŒ isTransQtl: Not available in your filtered dataset\")\n",
    "    \n",
    "    # Check for rightStudyType specifically\n",
    "    if 'rightStudyType' in tables_info.get('colocalisation_coloc', []):\n",
    "        select_columns.append(\"coloc.rightStudyType\")\n",
    "        print(f\"  âœ… rightStudyType: Available\")\n",
    "    else:\n",
    "        print(f\"  âŒ rightStudyType: Not available in your filtered dataset\")\n",
    "    \n",
    "    # Build the final query\n",
    "    select_clause = \",\\n      \".join(select_columns)\n",
    "    \n",
    "    specific_query = f\"\"\"\n",
    "    SELECT \n",
    "    {select_clause}                                    -- Dynamic column selection based on available data\n",
    "    FROM (\n",
    "    -- Step 1: Filter high-confidence colocalizations from main dataset\n",
    "    SELECT * FROM read_parquet('{output_path}/colocalisation_coloc.parquet')\n",
    "    WHERE h4 > 0.8                                     -- Only include strong colocalization evidence (>80% probability)\n",
    "    USING SAMPLE 10000 ROWS                            -- Random sample to prevent memory issues with large datasets\n",
    "    ) coloc\n",
    "    -- Step 2: Join with credible sets to get variant-level information\n",
    "    LEFT JOIN read_parquet('{output_path}/credible_set.parquet') credible\n",
    "    ON coloc.rightStudyLocusId = credible.studyLocusId  -- Link colocalization to fine-mapped variants\n",
    "    -- Step 3: Join with study metadata to get trait and publication information  \n",
    "    LEFT JOIN read_parquet('{output_path}/study.parquet') study\n",
    "    ON credible.studyId = study.studyId                  -- Link credible sets to study descriptions\n",
    "    WHERE credible.studyId IS NOT NULL                   -- Filter out colocalizations without credible set matches\n",
    "    ORDER BY coloc.h4 DESC                               -- Sort by colocalization confidence (highest first)\n",
    "    LIMIT 100                                            -- Return top 100 results for manageable analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nðŸš€ Executing query with {len(select_columns)} available columns...\")\n",
    "    print(f\"ðŸ“‹ Selected columns: {[col.split('.')[-1] for col in select_columns]}\")\n",
    "    \n",
    "    try:\n",
    "        df_specific = con.execute(specific_query).df()\n",
    "        print(f\"âœ… Dataset created: {len(df_specific)} rows\")\n",
    "        print(f\"ðŸ“‹ Final columns: {df_specific.columns.tolist()}\")\n",
    "        \n",
    "        # Data completeness analysis\n",
    "        print(f\"\\nðŸ“Š Data completeness:\")\n",
    "        for col in df_specific.columns:\n",
    "            non_null = df_specific[col].notna().sum()\n",
    "            print(f\"  {col}: {non_null}/{len(df_specific)} ({non_null/len(df_specific)*100:.1f}%)\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nðŸ‘€ Sample of your dataset:\")\n",
    "        print(df_specific.head())\n",
    "        \n",
    "        # Analyze numberColocalisingVariants if available\n",
    "        if 'numberColocalisingVariants' in df_specific.columns:\n",
    "            print(f\"\\nðŸ”— Colocalising Variants Analysis:\")\n",
    "            variants_stats = df_specific['numberColocalisingVariants'].describe()\n",
    "            print(f\"  Mean variants per colocalization: {variants_stats['mean']:.1f}\")\n",
    "            print(f\"  Median variants: {variants_stats['50%']:.1f}\")\n",
    "            print(f\"  Range: {variants_stats['min']:.0f} - {variants_stats['max']:.0f}\")\n",
    "            print(f\"  High variant count (>10): {(df_specific['numberColocalisingVariants'] > 10).sum()} pairs\")\n",
    "        \n",
    "        # Analyze variantId if available\n",
    "        if 'variantId' in df_specific.columns:\n",
    "            print(f\"\\nðŸ§¬ Variant ID Analysis:\")\n",
    "            unique_variants = df_specific['variantId'].nunique()\n",
    "            total_rows = len(df_specific)\n",
    "            print(f\"  Unique variants: {unique_variants:,}\")\n",
    "            print(f\"  Total rows: {total_rows:,}\")\n",
    "            print(f\"  Variants per row ratio: {unique_variants/total_rows:.2f}\")\n",
    "            \n",
    "            # Show some example variant IDs\n",
    "            sample_variants = df_specific['variantId'].dropna().head().tolist()\n",
    "            print(f\"  Example variant IDs: {sample_variants}\")\n",
    "        \n",
    "        # Analyze isTransQtl if available\n",
    "        if 'isTransQtl' in df_specific.columns:\n",
    "            print(f\"\\nðŸ§¬ Trans-QTL Analysis:\")\n",
    "            trans_qtl_count = df_specific['isTransQtl'].sum()\n",
    "            total_rows = len(df_specific)\n",
    "            print(f\"  Trans-QTL colocalizations: {trans_qtl_count}/{total_rows} ({trans_qtl_count/total_rows*100:.1f}%)\")\n",
    "            print(f\"  Cis-QTL colocalizations: {total_rows - trans_qtl_count}/{total_rows} ({(total_rows - trans_qtl_count)/total_rows*100:.1f}%)\")\n",
    "        \n",
    "        # Analyze colocalization evidence if h-values available\n",
    "        if available_h_cols:\n",
    "            print(f\"\\nðŸ§¬ Colocalization analysis:\")\n",
    "            print(f\"  Available evidence: {available_h_cols}\")\n",
    "            print(f\"  Mean posterior probabilities:\")\n",
    "            for h_col in available_h_cols:\n",
    "                if h_col in df_specific.columns:\n",
    "                    mean_val = df_specific[h_col].mean()\n",
    "                    print(f\"    {h_col}: {mean_val:.4f}\")\n",
    "        \n",
    "        # Study type analysis if available\n",
    "        if 'studyType' in df_specific.columns:\n",
    "            print(f\"\\nðŸ“Š Study type distribution:\")\n",
    "            study_counts = df_specific['studyType'].value_counts()\n",
    "            for study_type, count in study_counts.items():\n",
    "                print(f\"  {study_type}: {count} rows\")\n",
    "        \n",
    "        # Right study type analysis if available\n",
    "        if 'rightStudyType' in df_specific.columns:\n",
    "            print(f\"\\nðŸ“Š Right study type distribution:\")\n",
    "            right_study_counts = df_specific['rightStudyType'].value_counts()\n",
    "            for study_type, count in right_study_counts.items():\n",
    "                print(f\"  {study_type}: {count} rows\")\n",
    "        \n",
    "        # Cross-analysis if both variants and study types are available\n",
    "        if 'numberColocalisingVariants' in df_specific.columns and 'rightStudyType' in df_specific.columns:\n",
    "            print(f\"\\nðŸ”— Variants by right study type:\")\n",
    "            variants_by_type = df_specific.groupby('rightStudyType')['numberColocalisingVariants'].agg(['mean', 'count'])\n",
    "            for study_type, stats in variants_by_type.iterrows():\n",
    "                print(f\"  {study_type}: {stats['mean']:.1f} avg variants (n={stats['count']})\")\n",
    "        \n",
    "        return df_specific\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Query failed: {e}\")\n",
    "        print(f\"\\nQuery was:\")\n",
    "        print(specific_query)\n",
    "        return None\n",
    "    finally:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "da050b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTION FROM MAURO\n",
    "def analyze_gwas_colocalizations_with_available_columns_doubleJoin(output_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze GWAS-QTL colocalizations with dynamic column selection and comprehensive statistics.\n",
    "    \n",
    "    This function performs a robust analysis of genetic colocalization data by first checking\n",
    "    what columns are available in the filtered OpenTargets datasets, then building a dynamic\n",
    "    SQL query to extract maximum information. It focuses on high-confidence colocalizations\n",
    "    (H4 > 0.8) and provides detailed statistical summaries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    output_path : str\n",
    "        Path to directory containing filtered OpenTargets parquet files. Expected files:\n",
    "        - colocalisation_coloc.parquet : Colocalization evidence between study pairs\n",
    "        - credible_set.parquet : Fine-mapped credible sets for study loci  \n",
    "        - study.parquet : Study metadata including traits and types\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame or None\n",
    "        DataFrame containing colocalization analysis with available columns, or None if failed.\n",
    "        \n",
    "        Core columns (always present):\n",
    "        - leftStudyLocusId : Study-locus identifier for GWAS signal\n",
    "        - rightStudyLocusId : Study-locus identifier for QTL signal\n",
    "        - studyId : Study identifier from credible set\n",
    "        - chromosome : Chromosomal location of colocalization\n",
    "        \n",
    "        Optional columns (included if available in source data):\n",
    "        - numberColocalisingVariants : Number of variants supporting colocalization\n",
    "        - variantId : Lead variant identifier (rs ID or chr:pos format)\n",
    "        - isTransQtl : Boolean indicating trans-QTL (distant) vs cis-QTL (local)\n",
    "        - h0, h1, h2, h3, h4 : Posterior probabilities for colocalization hypotheses\n",
    "        - traitFromSource : Human-readable trait description\n",
    "        - studyType : Type of left-side study (gwas, eqtl, pqtl, etc.)\n",
    "        - rightStudyType : Type of right-side study (eqtl, pqtl, etc.)\n",
    "        \n",
    "    Algorithm\n",
    "    ---------\n",
    "    1. **Schema Detection**: Checks available columns in each parquet file\n",
    "    2. **Dynamic Query Building**: Constructs SQL with only available columns\n",
    "    3. **High-Confidence Filtering**: Selects colocalizations with H4 > 0.8\n",
    "    4. **Multi-table Join**: Links colocalization â†’ credible set â†’ study metadata\n",
    "    5. **Statistical Analysis**: Computes comprehensive summaries by column type\n",
    "    \n",
    "    Query Logic\n",
    "    -----------\n",
    "    ```sql\n",
    "    SELECT [dynamic_columns],\n",
    "       cs_right.studyLocusId   AS studyLocusId_right,\n",
    "       s_right.studyId         AS studyId_right,\n",
    "       s_right.traitReported   AS traitReported_right,\n",
    "       s_right.pubAuthor       AS pubAuthor_right,\n",
    "       s_right.pubJournal      AS pubJournal_right,\n",
    "       s_right.pubYear         AS pubYear_right,\n",
    "       cs_left.studyLocusId    AS studyLocusId_left,\n",
    "       s_left.studyId          AS studyId_left,\n",
    "       s_left.traitReported    AS traitReported_left,\n",
    "       s_left.pubAuthor        AS pubAuthor_left,\n",
    "       s_left.pubJournal       AS pubJournal_left,\n",
    "       s_left.pubYear          AS pubYear_left\n",
    "    FROM colocalisation_coloc c\n",
    "         LEFT JOIN credible_set cs_right ON c.rightStudyLocusId = cs_right.studyLocusId\n",
    "         LEFT JOIN study s_right         ON cs_right.studyId = s_right.studyId\n",
    "         LEFT JOIN credible_set cs_left  ON c.leftStudyLocusId = cs_left.studyLocusId\n",
    "         LEFT JOIN study s_left          ON cs_left.studyId = s_left.studyId\n",
    "    WHERE (s_right.studyId IS NOT NULL OR s_left.studyId IS NOT NULL)\n",
    "      AND c.H4 > 0.8 \n",
    "    ORDER BY c.H4 DESC\n",
    "    LIMIT 100;\n",
    "    ```\n",
    "    \n",
    "    Statistical Outputs\n",
    "    ------------------\n",
    "    - **Data Completeness**: Missing value analysis for each column\n",
    "    - **Variant Analysis**: Statistics on numberColocalisingVariants (mean, median, range)\n",
    "    - **Variant ID Analysis**: Uniqueness and example variant identifiers\n",
    "    - **Trans-QTL Analysis**: Proportion of trans vs cis regulatory effects\n",
    "    - **Colocalization Evidence**: Mean posterior probabilities (H0-H4)\n",
    "    - **Study Type Distribution**: Breakdown by GWAS, eQTL, pQTL types\n",
    "    - **Cross-tabulations**: Variants by study type combinations\n",
    "    \n",
    "    Colocalization Hypotheses (H-values)\n",
    "    ------------------------------------\n",
    "    - H0: Neither trait has genetic association at locus\n",
    "    - H1: Only left trait (typically GWAS) has association  \n",
    "    - H2: Only right trait (typically QTL) has association\n",
    "    - H3: Both traits associated, but different causal variants\n",
    "    - H4: Both traits associated, shared causal variant (colocalization)\n",
    "    \n",
    "    Memory Management\n",
    "    ----------------\n",
    "    - Uses DuckDB for efficient parquet processing\n",
    "    - Samples large datasets (10K rows) to prevent memory issues\n",
    "    - Automatic connection cleanup via context manager\n",
    "    \n",
    "    Error Handling\n",
    "    --------------\n",
    "    - Graceful handling of missing parquet files\n",
    "    - Column availability checking prevents SQL errors\n",
    "    - Detailed error reporting with query debugging information\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Basic usage\n",
    "    >>> df = analyze_gwas_colocalizations_with_available_columns('/path/to/filtered_data')\n",
    "    >>> print(f\"Found {len(df)} high-confidence colocalizations\")\n",
    "    \n",
    "    >>> # Check for specific analyses\n",
    "    >>> if 'isTransQtl' in df.columns:\n",
    "    >>>     trans_qtl_pct = df['isTransQtl'].mean() * 100\n",
    "    >>>     print(f\"Trans-QTL percentage: {trans_qtl_pct:.1f}%\")\n",
    "    \n",
    "    >>> # Filter by study type\n",
    "    >>> eqtl_colocs = df[df['rightStudyType'] == 'eqtl']\n",
    "    >>> print(f\"eQTL colocalizations: {len(eqtl_colocs)}\")\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - Function prints extensive diagnostic information during execution\n",
    "    - Designed to work with any subset of OpenTargets columns\n",
    "    - H4 > 0.8 threshold represents high-confidence shared causal variants\n",
    "    - Sample size (10K rows) balances comprehensiveness with performance\n",
    "    - Results ordered by H4 (highest confidence first)\n",
    "    \n",
    "    See Also\n",
    "    --------\n",
    "    - OpenTargets Genetics documentation: https://genetics-docs.opentargets.org/\n",
    "    - Colocalization methods: Giambartolomei et al. (2014) PLoS Genet\n",
    "    - COLOC R package: https://github.com/chr1swallace/coloc\n",
    "    \"\"\"\n",
    "    \n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    print(\"ðŸ” Checking what columns are actually available in your filtered datasets...\")\n",
    "    \n",
    "    # Check available columns in each table\n",
    "    tables_info = {}\n",
    "    \n",
    "    for table_name in ['colocalisation_coloc', 'credible_set', 'study']:\n",
    "        try:\n",
    "            query = f\"SELECT * FROM read_parquet('{output_path}/{table_name}.parquet') LIMIT 1\"\n",
    "            sample = con.execute(query).df()\n",
    "            tables_info[table_name] = sample.columns.tolist()\n",
    "            print(f\"  ðŸ“‹ {table_name}: {sample.columns.tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {table_name}: Error - {e}\")\n",
    "            tables_info[table_name] = []\n",
    "    \n",
    "       # Core columns that should always be there\n",
    "    select_columns = [\n",
    "        \"coloc.leftStudyLocusId\",\n",
    "        \"coloc.rightStudyLocusId\",\n",
    "        \"credible_right.studyId AS studyId_right\", \n",
    "        \"credible_left.studyId AS studyId_left\",\n",
    "        \"coloc.chromosome\"\n",
    "    ]\n",
    "    \n",
    "    # Add h-values if available\n",
    "    h_columns = ['h0', 'h1', 'h2', 'h3', 'h4']\n",
    "    for h_col in h_columns:\n",
    "        if h_col in tables_info.get('colocalisation_coloc', []):\n",
    "            select_columns.append(f\"coloc.{h_col}\")\n",
    "    \n",
    "    # Add right-side study metadata\n",
    "    study_cols = ['traitFromSource', 'studyType']\n",
    "    for col in study_cols:\n",
    "        if col in tables_info.get('study', []):\n",
    "            select_columns.append(f\"study_right.{col} AS {col}_right\")\n",
    "            select_columns.append(f\"study_left.{col} AS {col}_left\")\n",
    "    \n",
    "    # Add numberColocalisingVariants if available\n",
    "    if 'numberColocalisingVariants' in tables_info.get('colocalisation_coloc', []):\n",
    "        select_columns.append(\"coloc.numberColocalisingVariants\")\n",
    "    \n",
    "    # Add variantId if available\n",
    "    if 'variantId' in tables_info.get('credible_set', []):\n",
    "        select_columns.append(\"credible_right.variantId AS variantId_right\")\n",
    "        select_columns.append(\"credible_left.variantId AS variantId_left\")\n",
    "    \n",
    "    # Add isTransQtl if available\n",
    "    if 'isTransQtl' in tables_info.get('credible_set', []):\n",
    "        select_columns.append(\"credible_right.isTransQtl AS isTransQtl_right\")\n",
    "        select_columns.append(\"credible_left.isTransQtl AS isTransQtl_left\")\n",
    "    \n",
    "    # Add rightStudyType if available\n",
    "    if 'rightStudyType' in tables_info.get('colocalisation_coloc', []):\n",
    "        select_columns.append(\"coloc.rightStudyType\")\n",
    "    \n",
    "    # Build the final query\n",
    "    select_clause = \",\\n      \".join(select_columns)\n",
    "    \n",
    "    specific_query = f\"\"\"\n",
    "    SELECT \n",
    "        {select_clause}\n",
    "    FROM (\n",
    "        SELECT * FROM read_parquet('{output_path}/colocalisation_coloc.parquet')\n",
    "        WHERE h4 > 0.8 \n",
    "            AND rightStudyType <> 'gwas'                     -- ðŸš« Exclude GWAS on right side (pre-filter!)\n",
    "        USING SAMPLE 10000 ROWS\n",
    "    ) coloc\n",
    "    -- Right-side join\n",
    "    LEFT JOIN read_parquet('{output_path}/credible_set.parquet') credible_right\n",
    "        ON coloc.rightStudyLocusId = credible_right.studyLocusId\n",
    "    LEFT JOIN read_parquet('{output_path}/study.parquet') study_right\n",
    "        ON credible_right.studyId = study_right.studyId\n",
    "    -- Left-side join\n",
    "    LEFT JOIN read_parquet('{output_path}/credible_set.parquet') credible_left\n",
    "        ON coloc.leftStudyLocusId = credible_left.studyLocusId\n",
    "    LEFT JOIN read_parquet('{output_path}/study.parquet') study_left\n",
    "        ON credible_left.studyId = study_left.studyId\n",
    "    WHERE credible_right.studyId IS NOT NULL \n",
    "       OR credible_left.studyId IS NOT NULL\n",
    "       OR coloc.rightStudyType <> 'gwas'  \n",
    "    ORDER BY coloc.h4 DESC\n",
    "    LIMIT 100\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nðŸš€ Executing query with {len(select_columns)} available columns...\")\n",
    "    print(f\"ðŸ“‹ Selected columns: {[col.split('.')[-1] for col in select_columns]}\")\n",
    "    \n",
    "    try:\n",
    "        df_specific = con.execute(specific_query).df()\n",
    "        print(f\"âœ… Dataset created: {len(df_specific)} rows\")\n",
    "        print(f\"ðŸ“‹ Final columns: {df_specific.columns.tolist()}\")\n",
    "        \n",
    "        # Data completeness analysis\n",
    "        print(f\"\\nðŸ“Š Data completeness:\")\n",
    "        for col in df_specific.columns:\n",
    "            non_null = df_specific[col].notna().sum()\n",
    "            print(f\"  {col}: {non_null}/{len(df_specific)} ({non_null/len(df_specific)*100:.1f}%)\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nðŸ‘€ Sample of your dataset:\")\n",
    "        print(df_specific.head())\n",
    "        \n",
    "        # Analyze numberColocalisingVariants if available\n",
    "        if 'numberColocalisingVariants' in df_specific.columns:\n",
    "            print(f\"\\nðŸ”— Colocalising Variants Analysis:\")\n",
    "            variants_stats = df_specific['numberColocalisingVariants'].describe()\n",
    "            print(f\"  Mean variants per colocalization: {variants_stats['mean']:.1f}\")\n",
    "            print(f\"  Median variants: {variants_stats['50%']:.1f}\")\n",
    "            print(f\"  Range: {variants_stats['min']:.0f} - {variants_stats['max']:.0f}\")\n",
    "            print(f\"  High variant count (>10): {(df_specific['numberColocalisingVariants'] > 10).sum()} pairs\")\n",
    "        \n",
    "        # Analyze variantId if available\n",
    "        if 'variantId' in df_specific.columns:\n",
    "            print(f\"\\nðŸ§¬ Variant ID Analysis:\")\n",
    "            unique_variants = df_specific['variantId'].nunique()\n",
    "            total_rows = len(df_specific)\n",
    "            print(f\"  Unique variants: {unique_variants:,}\")\n",
    "            print(f\"  Total rows: {total_rows:,}\")\n",
    "            print(f\"  Variants per row ratio: {unique_variants/total_rows:.2f}\")\n",
    "            \n",
    "            # Show some example variant IDs\n",
    "            sample_variants = df_specific['variantId'].dropna().head().tolist()\n",
    "            print(f\"  Example variant IDs: {sample_variants}\")\n",
    "        \n",
    "        # Analyze isTransQtl if available\n",
    "        if 'isTransQtl' in df_specific.columns:\n",
    "            print(f\"\\nðŸ§¬ Trans-QTL Analysis:\")\n",
    "            trans_qtl_count = df_specific['isTransQtl'].sum()\n",
    "            total_rows = len(df_specific)\n",
    "            print(f\"  Trans-QTL colocalizations: {trans_qtl_count}/{total_rows} ({trans_qtl_count/total_rows*100:.1f}%)\")\n",
    "            print(f\"  Cis-QTL colocalizations: {total_rows - trans_qtl_count}/{total_rows} ({(total_rows - trans_qtl_count)/total_rows*100:.1f}%)\")\n",
    "        \n",
    "        # Analyze colocalization evidence if h-values available\n",
    "       # if available_h_cols:\n",
    "        #    print(f\"\\nðŸ§¬ Colocalization analysis:\")\n",
    "       #     print(f\"  Available evidence: {available_h_cols}\")\n",
    "        #    print(f\"  Mean posterior probabilities:\")\n",
    "        #    for h_col in available_h_cols:\n",
    "        #        if h_col in df_specific.columns:\n",
    "        #            mean_val = df_specific[h_col].mean()\n",
    "        #            print(f\"    {h_col}: {mean_val:.4f}\")\n",
    "        \n",
    "        # Study type analysis if available\n",
    "        if 'studyType' in df_specific.columns:\n",
    "            print(f\"\\nðŸ“Š Study type distribution:\")\n",
    "            study_counts = df_specific['studyType'].value_counts()\n",
    "            for study_type, count in study_counts.items():\n",
    "                print(f\"  {study_type}: {count} rows\")\n",
    "        \n",
    "        # Right study type analysis if available\n",
    "        if 'rightStudyType' in df_specific.columns:\n",
    "            print(f\"\\nðŸ“Š Right study type distribution:\")\n",
    "            right_study_counts = df_specific['rightStudyType'].value_counts()\n",
    "            for study_type, count in right_study_counts.items():\n",
    "                print(f\"  {study_type}: {count} rows\")\n",
    "        \n",
    "        # Cross-analysis if both variants and study types are available\n",
    "        if 'numberColocalisingVariants' in df_specific.columns and 'rightStudyType' in df_specific.columns:\n",
    "            print(f\"\\nðŸ”— Variants by right study type:\")\n",
    "            variants_by_type = df_specific.groupby('rightStudyType')['numberColocalisingVariants'].agg(['mean', 'count'])\n",
    "            for study_type, stats in variants_by_type.iterrows():\n",
    "                print(f\"  {study_type}: {stats['mean']:.1f} avg variants (n={stats['count']})\")\n",
    "        \n",
    "        return df_specific\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Query failed: {e}\")\n",
    "        print(f\"\\nQuery was:\")\n",
    "        print(specific_query)\n",
    "        return None\n",
    "    finally:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "834d1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gene_drug_target_dataframe(output_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a dataframe linking genes to drug targets using OpenTargets data.\n",
    "    \n",
    "    This function combines multiple datasets to create comprehensive gene-drug-target mappings:\n",
    "    - l2g_prediction: Links genetic loci to genes (geneId)\n",
    "    - evidence: Links targets to diseases (targetId) \n",
    "    - known_drug: Links drugs to targets (drugId, targetId)\n",
    "    - target: Provides gene symbols and annotations\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    output_path : str\n",
    "        Path to directory containing filtered OpenTargets parquet files\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with gene and drug target information\n",
    "    \"\"\"\n",
    "    \n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    print(\"ðŸ” Checking available datasets for gene-drug-target mapping...\")\n",
    "    \n",
    "    # Check available columns in each relevant table\n",
    "    tables_info = {}\n",
    "    required_tables = ['l2g_prediction', 'evidence', 'known_drug', 'target']\n",
    "    \n",
    "    for table_name in required_tables:\n",
    "        try:\n",
    "            query = f\"SELECT * FROM read_parquet('{output_path}/{table_name}.parquet') LIMIT 1\"\n",
    "            sample = con.execute(query).df()\n",
    "            tables_info[table_name] = sample.columns.tolist()\n",
    "            print(f\"  ðŸ“‹ {table_name}: {sample.columns.tolist()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {table_name}: Error - {e}\")\n",
    "            tables_info[table_name] = []\n",
    "    \n",
    "    # Build dynamic query based on available columns\n",
    "    print(f\"\\nðŸ”§ Building gene-drug-target query...\")\n",
    "    \n",
    "    # Core columns we want to extract\n",
    "    select_columns = []\n",
    "    \n",
    "    # From l2g_prediction: geneId, studyLocusId, score\n",
    "    if 'geneId' in tables_info.get('l2g_prediction', []):\n",
    "        select_columns.extend([\n",
    "            \"l2g.geneId\",\n",
    "            \"l2g.studyLocusId\", \n",
    "            \"l2g.score as l2g_score\"\n",
    "        ])\n",
    "        print(f\"  âœ… l2g_prediction: geneId available\")\n",
    "    \n",
    "    # From evidence: targetId, diseaseId\n",
    "    if 'targetId' in tables_info.get('evidence', []):\n",
    "        select_columns.extend([\n",
    "            \"evidence.targetId\",\n",
    "            \"evidence.diseaseId\"\n",
    "        ])\n",
    "        print(f\"  âœ… evidence: targetId available\")\n",
    "    \n",
    "    # From known_drug: drugId and available columns (check what actually exists)\n",
    "    known_drug_cols = []\n",
    "    if 'drugId' in tables_info.get('known_drug', []):\n",
    "        known_drug_cols.append(\"drug.drugId\")\n",
    "        \n",
    "        # Check for optional columns that might exist\n",
    "        if 'phase' in tables_info.get('known_drug', []):\n",
    "            known_drug_cols.append(\"drug.phase\")\n",
    "        if 'status' in tables_info.get('known_drug', []):\n",
    "            known_drug_cols.append(\"drug.status\")\n",
    "            \n",
    "        select_columns.extend(known_drug_cols)\n",
    "        print(f\"  âœ… known_drug: {[col.split('.')[-1] for col in known_drug_cols]} available\")\n",
    "    \n",
    "    # From target: gene symbol and annotations\n",
    "    target_cols = []\n",
    "    if 'approvedSymbol' in tables_info.get('target', []):\n",
    "        target_cols.append(\"target.approvedSymbol as gene_symbol\")\n",
    "    if 'biotype' in tables_info.get('target', []):\n",
    "        target_cols.append(\"target.biotype\")\n",
    "    \n",
    "    if target_cols:\n",
    "        select_columns.extend(target_cols)\n",
    "        print(f\"  âœ… target: {[col.split('.')[-1].split(' as ')[-1] for col in target_cols]} available\")\n",
    "    \n",
    "    if not select_columns:\n",
    "        print(\"âŒ No required columns found in datasets\")\n",
    "        return None\n",
    "    \n",
    "    # Build the comprehensive query\n",
    "    select_clause = \",\\n      \".join(select_columns)\n",
    "    \n",
    "    gene_drug_query = f\"\"\"\n",
    "    SELECT \n",
    "      {select_clause}\n",
    "    FROM (\n",
    "      -- Start with locus-to-gene predictions (high confidence genes)\n",
    "      SELECT * FROM read_parquet('{output_path}/l2g_prediction.parquet')\n",
    "      WHERE score > 0.5  -- Only include high-confidence gene predictions\n",
    "      USING SAMPLE 50000 ROWS  -- Sample to manage memory\n",
    "    ) l2g\n",
    "    -- Join with evidence to get target-disease associations\n",
    "    LEFT JOIN read_parquet('{output_path}/evidence.parquet') evidence\n",
    "      ON l2g.geneId = evidence.targetId  -- Gene ID = Target ID in OpenTargets\n",
    "    -- Join with known drugs to get drug information\n",
    "    LEFT JOIN read_parquet('{output_path}/known_drug.parquet') drug\n",
    "      ON evidence.targetId = drug.targetId \n",
    "      AND evidence.diseaseId = drug.diseaseId  -- Match target-disease pairs\n",
    "    -- Join with target info to get gene symbols\n",
    "    LEFT JOIN read_parquet('{output_path}/target.parquet') target\n",
    "      ON l2g.geneId = target.id  -- Gene ID matches target ID\n",
    "    WHERE evidence.targetId IS NOT NULL  -- Ensure we have target information\n",
    "      AND drug.drugId IS NOT NULL        -- Ensure we have drug information\n",
    "    ORDER BY l2g.score DESC  -- Prioritize high-confidence genes\n",
    "    LIMIT 1000  -- Return top 1000 gene-drug-target associations\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nðŸš€ Executing gene-drug-target query...\")\n",
    "    print(f\"ðŸ“‹ Selected columns: {[col.split('.')[-1].split(' as ')[-1] for col in select_columns]}\")\n",
    "    \n",
    "    try:\n",
    "        df_gene_drug = con.execute(gene_drug_query).df()\n",
    "        print(f\"âœ… Dataset created: {len(df_gene_drug)} rows\")\n",
    "        print(f\"ðŸ“‹ Final columns: {df_gene_drug.columns.tolist()}\")\n",
    "        \n",
    "        # Data summary\n",
    "        print(f\"\\nðŸ“Š Gene-Drug-Target Summary:\")\n",
    "        if 'geneId' in df_gene_drug.columns:\n",
    "            unique_genes = df_gene_drug['geneId'].nunique()\n",
    "            print(f\"  Unique genes: {unique_genes:,}\")\n",
    "        \n",
    "        if 'drugId' in df_gene_drug.columns:\n",
    "            unique_drugs = df_gene_drug['drugId'].nunique()\n",
    "            print(f\"  Unique drugs: {unique_drugs:,}\")\n",
    "        \n",
    "        if 'targetId' in df_gene_drug.columns:\n",
    "            unique_targets = df_gene_drug['targetId'].nunique()\n",
    "            print(f\"  Unique targets: {unique_targets:,}\")\n",
    "        \n",
    "        # Clinical phase analysis (only if phase column exists)\n",
    "        if 'phase' in df_gene_drug.columns:\n",
    "            print(f\"\\nðŸ’Š Drug Development Phases:\")\n",
    "            phase_counts = df_gene_drug['phase'].value_counts().sort_index()\n",
    "            for phase, count in phase_counts.items():\n",
    "                phase_name = {0: \"Preclinical\", 1: \"Phase I\", 2: \"Phase II\", \n",
    "                            3: \"Phase III\", 4: \"Approved\"}.get(phase, f\"Phase {phase}\")\n",
    "                print(f\"  {phase_name}: {count} associations\")\n",
    "        \n",
    "        # L2G score analysis\n",
    "        if 'l2g_score' in df_gene_drug.columns:\n",
    "            print(f\"\\nðŸŽ¯ Locus-to-Gene Score Distribution:\")\n",
    "            score_stats = df_gene_drug['l2g_score'].describe()\n",
    "            print(f\"  Mean score: {score_stats['mean']:.3f}\")\n",
    "            print(f\"  High confidence (>0.8): {(df_gene_drug['l2g_score'] > 0.8).sum()} genes\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nðŸ‘€ Sample gene-drug-target associations:\")\n",
    "        display_cols = ['geneId', 'gene_symbol', 'drugId', 'targetId', 'l2g_score']\n",
    "        if 'phase' in df_gene_drug.columns:\n",
    "            display_cols.insert(-1, 'phase')\n",
    "        available_display_cols = [col for col in display_cols if col in df_gene_drug.columns]\n",
    "        print(df_gene_drug[available_display_cols].head())\n",
    "        \n",
    "        return df_gene_drug\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Query failed: {e}\")\n",
    "        print(f\"\\nQuery was:\")\n",
    "        print(gene_drug_query)\n",
    "        return None\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "# Usage example - make sure to define the paths\n",
    "def run_gene_drug_analysis():\n",
    "    \"\"\"Run the gene-drug-target analysis and save results.\"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ Creating gene-drug-target dataframe...\")\n",
    "    df_gene_drug = create_gene_drug_target_dataframe(output_path)\n",
    "    \n",
    "    if df_gene_drug is not None:\n",
    "        print(f\"\\nðŸŽ‰ Analysis complete!\")\n",
    "        print(f\"ðŸ“Š Generated {len(df_gene_drug)} gene-drug-target associations\")\n",
    "        \n",
    "        # Save the dataset\n",
    "        output_file = main_dir / \"gene_drug_target_associations.csv\"\n",
    "        df_gene_drug.to_csv(output_file, index=False)\n",
    "        print(f\"ðŸ’¾ Saved to: {output_file}\")\n",
    "        \n",
    "        # Additional analysis\n",
    "        print(f\"\\nðŸ”¬ Key Insights:\")\n",
    "        if 'gene_symbol' in df_gene_drug.columns and 'drugId' in df_gene_drug.columns:\n",
    "            # Top genes by drug count\n",
    "            top_genes = df_gene_drug.groupby('gene_symbol')['drugId'].nunique().sort_values(ascending=False).head()\n",
    "            print(f\"  Top genes by drug count:\")\n",
    "            for gene, drug_count in top_genes.items():\n",
    "                print(f\"    {gene}: {drug_count} drugs\")\n",
    "        \n",
    "        return df_gene_drug\n",
    "    else:\n",
    "        print(\"âŒ Failed to create gene-drug-target dataframe\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bfcd58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = Path(\"/lustre/groups/itg/shared/referenceData/OpenTargets/\")\n",
    "\n",
    "base_path = main_dir / \"data_version_29_07\"\n",
    "output_path = main_dir / \"temp/01_filtered_parquets\"\n",
    "manifest_path = main_dir / \"temp/columns_manifest.json\"\n",
    "\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the columns manifest to get the list of datasets and their columns\n",
    "with manifest_path.open() as f:\n",
    "    manifest = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52906ae",
   "metadata": {},
   "source": [
    "### Batch Processing of OpenTargets Datasets with Memory-Efficient Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04a398eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Process 8 datasets\n",
      "\n",
      "============================================================\n",
      "Processing colocalisation_coloc (1/8)\n",
      "============================================================\n",
      "\n",
      "ðŸ”„ Processing colocalisation_coloc...\n",
      "ðŸ“‹ Available columns (12): ['leftStudyLocusId', 'rightStudyLocusId', 'chromosome', 'rightStudyType', 'numberColocalisingVariants', 'h0', 'h1', 'h2', 'h3', 'h4']...\n",
      "ðŸ”„ Converting to table with 10 columns: ['leftStudyLocusId', 'rightStudyLocusId', 'chromosome', 'h4', 'rightStudyType', 'numberColocalisingVariants', 'h0', 'h1', 'h2', 'h3']\n",
      "ðŸ“Š Large dataset detected, using batch processing...\n",
      "  ðŸ“Š Processed 10 batches, 5,626,147 rows so far...\n",
      "  ðŸ“Š Processed 20 batches, 10,630,414 rows so far...\n",
      "  ðŸ“Š Processed 30 batches, 15,424,575 rows so far...\n",
      "  ðŸ“Š Processed 40 batches, 21,052,744 rows so far...\n",
      "  ðŸ“Š Processed 50 batches, 26,057,870 rows so far...\n",
      "  ðŸ“Š Processed 60 batches, 30,844,974 rows so far...\n",
      "  ðŸ“Š Processed 70 batches, 36,474,946 rows so far...\n",
      "âœ… Saved colocalisation_coloc: 38,561,709 rows, 10 columns (38.9s)\n",
      "Memory cleanup completed.\n",
      "\n",
      "============================================================\n",
      "Processing credible_set (2/8)\n",
      "============================================================\n",
      "\n",
      "ðŸ”„ Processing credible_set...\n",
      "ðŸ“‹ Available columns (27): ['studyLocusId', 'studyId', 'variantId', 'chromosome', 'position', 'region', 'beta', 'zScore', 'pValueMantissa', 'pValueExponent']...\n",
      "ðŸ”„ Converting to table with 4 columns: ['studyLocusId', 'studyId', 'variantId', 'isTransQtl']\n",
      "ðŸ“Š Large dataset detected, using batch processing...\n",
      "  ðŸ“Š Processed 10 batches, 1,130,069 rows so far...\n",
      "  ðŸ“Š Processed 20 batches, 2,129,374 rows so far...\n",
      "âœ… Saved credible_set: 2,833,758 rows, 4 columns (3.0s)\n",
      "Memory cleanup completed.\n",
      "\n",
      "============================================================\n",
      "Processing l2g_prediction (3/8)\n",
      "============================================================\n",
      "\n",
      "ðŸ”„ Processing l2g_prediction...\n",
      "ðŸ“‹ Available columns (5): ['studyLocusId', 'geneId', 'score', 'features', 'shapBaseValue']\n",
      "ðŸ”„ Converting to table with 3 columns: ['studyLocusId', 'geneId', 'score']\n",
      "âœ… Saved l2g_prediction: 1,473,532 rows, 3 columns (1.5s)\n",
      "Memory cleanup completed.\n",
      "\n",
      "============================================================\n",
      "Processing evidence (4/8)\n",
      "============================================================\n",
      "\n",
      "ðŸ”„ Processing evidence...\n",
      "ðŸ“‹ Available columns (88): ['datasourceId', 'targetId', 'alleleOrigins', 'allelicRequirements', 'ancestry', 'ancestryId', 'beta', 'betaConfidenceIntervalLower', 'betaConfidenceIntervalUpper', 'biologicalModelAllelicComposition']...\n",
      "ðŸ”„ Converting to table with 3 columns: ['studyId', 'targetId', 'diseaseId']\n",
      "âœ… Saved evidence: 29,602,753 rows, 3 columns (71.9s)\n",
      "Memory cleanup completed.\n",
      "\n",
      "============================================================\n",
      "Processing known_drug (5/8)\n",
      "============================================================\n",
      "\n",
      "ðŸ”„ Processing known_drug...\n",
      "ðŸ“‹ Available columns (17): ['drugId', 'targetId', 'diseaseId', 'phase', 'status', 'urls', 'ancestors', 'label', 'approvedSymbol', 'approvedName']...\n",
      "ðŸ”„ Converting to table with 3 columns: ['targetId', 'diseaseId', 'drugId']\n",
      "âœ… Saved known_drug: 253,442 rows, 3 columns (0.2s)\n",
      "Memory cleanup completed.\n",
      "\n",
      "============================================================\n",
      "Processing disease (6/8)\n",
      "============================================================\n",
      "\n",
      "ðŸ”„ Processing disease...\n",
      "ðŸ“‹ Available columns (14): ['id', 'code', 'name', 'description', 'dbXRefs', 'parents', 'synonyms', 'obsoleteTerms', 'obsoleteXRefs', 'children']...\n",
      "ðŸ”„ Converting to table with 3 columns: ['id', 'name', 'description']\n",
      "âœ… Saved disease: 38,959 rows, 3 columns (0.2s)\n",
      "Memory cleanup completed.\n",
      "\n",
      "============================================================\n",
      "Processing study (7/8)\n",
      "============================================================\n",
      "\n",
      "ðŸ”„ Processing study...\n",
      "ðŸ“‹ Available columns (30): ['studyId', 'geneId', 'projectId', 'studyType', 'traitFromSource', 'traitFromSourceMappedIds', 'biosampleFromSourceId', 'pubmedId', 'publicationTitle', 'publicationFirstAuthor']...\n",
      "ðŸ”„ Converting to table with 6 columns: ['studyId', 'studyType', 'traitFromSource', 'projectId', 'pubmedId', 'geneId']\n",
      "âœ… Saved study: 1,966,178 rows, 6 columns (1.5s)\n",
      "Memory cleanup completed.\n",
      "\n",
      "============================================================\n",
      "Processing target (8/8)\n",
      "============================================================\n",
      "\n",
      "ðŸ”„ Processing target...\n",
      "ðŸ“‹ Available columns (29): ['id', 'approvedSymbol', 'biotype', 'transcriptIds', 'canonicalTranscript', 'canonicalExons', 'genomicLocation', 'alternativeGenes', 'approvedName', 'go']...\n",
      "ðŸ”„ Converting to table with 3 columns: ['id', 'approvedSymbol', 'biotype']\n",
      "âœ… Saved target: 78,726 rows, 3 columns (0.1s)\n",
      "Memory cleanup completed.\n",
      "\n",
      "ðŸŽ‰ Processing complete!\n",
      "âœ… Successfully processed: 8/8 datasets\n",
      "ðŸ“‚ Filtered datasets saved to: /lustre/groups/itg/shared/referenceData/OpenTargets/temp/01_filtered_parquets\n"
     ]
    }
   ],
   "source": [
    "print(f\"ðŸ“‹ Process {len(manifest)} datasets\")\n",
    "\n",
    "# Process datasets one by one to avoid memory issues\n",
    "success_count = 0\n",
    "failed_datasets = []\n",
    "\n",
    "# Process large datasets first (they're most likely to cause memory issues)\n",
    "large_datasets = ['colocalisation_coloc', 'credible_set']\n",
    "small_datasets = [name for name in manifest.keys() if name not in large_datasets]\n",
    "\n",
    "# Order: large datasets first, then small ones\n",
    "processing_order = large_datasets + small_datasets\n",
    "\n",
    "for dataset_name in processing_order:\n",
    "    if dataset_name in manifest:\n",
    "        columns = manifest[dataset_name]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing {dataset_name} ({success_count + 1}/{len(manifest)})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        success = process_single_dataset(dataset_name, columns, base_path, output_path)\n",
    "        \n",
    "        if success:\n",
    "            success_count += 1\n",
    "        else:\n",
    "            failed_datasets.append(dataset_name)\n",
    "        \n",
    "        # Force garbage collection between datasets\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Memory cleanup completed.\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Processing complete!\")\n",
    "print(f\"âœ… Successfully processed: {success_count}/{len(manifest)} datasets\")\n",
    "\n",
    "if failed_datasets:\n",
    "    print(f\"âŒ Failed datasets: {failed_datasets}\")\n",
    "\n",
    "print(f\"ðŸ“‚ Filtered datasets saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4f20928f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Checking what columns are actually available in your filtered datasets...\n",
      "  ðŸ“‹ colocalisation_coloc: ['leftStudyLocusId', 'rightStudyLocusId', 'chromosome', 'h4', 'rightStudyType', 'numberColocalisingVariants', 'h0', 'h1', 'h2', 'h3']\n",
      "  ðŸ“‹ credible_set: ['studyLocusId', 'studyId', 'variantId', 'isTransQtl']\n",
      "  ðŸ“‹ study: ['studyId', 'studyType', 'traitFromSource', 'projectId', 'pubmedId', 'geneId']\n",
      "\n",
      "ðŸš€ Executing query with 20 available columns...\n",
      "ðŸ“‹ Selected columns: ['leftStudyLocusId', 'rightStudyLocusId', 'studyId AS studyId_right', 'studyId AS studyId_left', 'chromosome', 'h0', 'h1', 'h2', 'h3', 'h4', 'traitFromSource AS traitFromSource_right', 'traitFromSource AS traitFromSource_left', 'studyType AS studyType_right', 'studyType AS studyType_left', 'numberColocalisingVariants', 'variantId AS variantId_right', 'variantId AS variantId_left', 'isTransQtl AS isTransQtl_right', 'isTransQtl AS isTransQtl_left', 'rightStudyType']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf672d4a73b4b94a2de948326f4ecbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset created: 100 rows\n",
      "ðŸ“‹ Final columns: ['leftStudyLocusId', 'rightStudyLocusId', 'studyId_right', 'studyId_left', 'chromosome', 'h0', 'h1', 'h2', 'h3', 'h4', 'traitFromSource_right', 'traitFromSource_left', 'studyType_right', 'studyType_left', 'numberColocalisingVariants', 'variantId_right', 'variantId_left', 'isTransQtl_right', 'isTransQtl_left', 'rightStudyType']\n",
      "\n",
      "ðŸ“Š Data completeness:\n",
      "  leftStudyLocusId: 100/100 (100.0%)\n",
      "  rightStudyLocusId: 100/100 (100.0%)\n",
      "  studyId_right: 100/100 (100.0%)\n",
      "  studyId_left: 100/100 (100.0%)\n",
      "  chromosome: 100/100 (100.0%)\n",
      "  h0: 100/100 (100.0%)\n",
      "  h1: 100/100 (100.0%)\n",
      "  h2: 100/100 (100.0%)\n",
      "  h3: 100/100 (100.0%)\n",
      "  h4: 100/100 (100.0%)\n",
      "  traitFromSource_right: 100/100 (100.0%)\n",
      "  traitFromSource_left: 100/100 (100.0%)\n",
      "  studyType_right: 100/100 (100.0%)\n",
      "  studyType_left: 100/100 (100.0%)\n",
      "  numberColocalisingVariants: 100/100 (100.0%)\n",
      "  variantId_right: 100/100 (100.0%)\n",
      "  variantId_left: 100/100 (100.0%)\n",
      "  isTransQtl_right: 100/100 (100.0%)\n",
      "  isTransQtl_left: 0/100 (0.0%)\n",
      "  rightStudyType: 100/100 (100.0%)\n",
      "\n",
      "ðŸ‘€ Sample of your dataset:\n",
      "                   leftStudyLocusId                 rightStudyLocusId  \\\n",
      "0  93cd7d9317ba066bbd0183f20d4414d6  aee53ca025534856cb3ed77e7da8e73d   \n",
      "1  49ee6904ebd90b94a55130e956c4ee37  e7ec02bede2ef556374be1a7dc006e8b   \n",
      "2  3ca4de9e33824bfab394b504a69e7919  994846b0cd95fd3230bedaf598a42933   \n",
      "3  0d050577abf1dd125314b534450c94bb  4fe83a00a4c5f8a03dfc96b94e0f3a3c   \n",
      "4  6d9920a668f3227bc8b6e65ed7518b02  20a1eb0626cd2cbb09b01702230bcaa8   \n",
      "\n",
      "                                       studyId_right  studyId_left chromosome  \\\n",
      "0                UKB_PPP_EUR_MICB_Q29980_OID20593_v1  GCST90445899         17   \n",
      "1      schmiedel_2018_ge_treg_memory_ensg00000113504  GCST90479673          5   \n",
      "2  fairfax_2014_microarray_monocyte_ifn24_ilmn_17...  GCST90445970          4   \n",
      "3                UKB_PPP_EUR_DSG3_P32926_OID21460_v1  GCST90445988          2   \n",
      "4                 UKB_PPP_EUR_LPA_P08519_OID30747_v1  GCST90319603          6   \n",
      "\n",
      "             h0             h1            h2            h3   h4  \\\n",
      "0  1.878535e-89   1.990494e-33  9.437535e-60  1.878535e-97  1.0   \n",
      "1  7.052834e-72   4.592746e-54  1.535647e-21  7.052834e-80  1.0   \n",
      "2  0.000000e+00  4.316137e-146  0.000000e+00  0.000000e+00  1.0   \n",
      "3  0.000000e+00   5.173614e-25  0.000000e+00  0.000000e+00  1.0   \n",
      "4  0.000000e+00   0.000000e+00  1.052813e-68  0.000000e+00  1.0   \n",
      "\n",
      "                 traitFromSource_right  \\\n",
      "0  MICB_MICA:Q29980_Q29983:OID20593:v1   \n",
      "1                      ENSG00000113504   \n",
      "2                         ILMN_1793017   \n",
      "3              DSG3:P32926:OID21460:v1   \n",
      "4               LPA:P08519:OID30747:v1   \n",
      "\n",
      "                                traitFromSource_left studyType_right  \\\n",
      "0        Phospholipids in LDL (UKB data field 23413)            pqtl   \n",
      "1  mean corpuscular hemoglobin (MCH, mean, inv-no...            eqtl   \n",
      "2  Free Cholesterol to Cholesteryl Esters in Medi...            eqtl   \n",
      "3  Free cholesterol in medium VLDL (UKB data fiel...            pqtl   \n",
      "4                      Serum apolipoprotein B levels            pqtl   \n",
      "\n",
      "  studyType_left  numberColocalisingVariants  variantId_right  \\\n",
      "0           gwas                           1   17_7166093_G_A   \n",
      "1           gwas                           1    5_1104823_C_T   \n",
      "2           gwas                           1     4_973551_C_G   \n",
      "3           gwas                           1   2_27508073_T_C   \n",
      "4           gwas                           1  6_160591981_T_C   \n",
      "\n",
      "    variantId_left  isTransQtl_right  isTransQtl_left rightStudyType  \n",
      "0   17_7166093_G_A              True             <NA>           pqtl  \n",
      "1    5_1104823_C_T             False             <NA>           eqtl  \n",
      "2     4_973551_C_G             False             <NA>           eqtl  \n",
      "3   2_27508073_T_C              True             <NA>           pqtl  \n",
      "4  6_160591981_T_C             False             <NA>           pqtl  \n",
      "\n",
      "ðŸ”— Colocalising Variants Analysis:\n",
      "  Mean variants per colocalization: 1.0\n",
      "  Median variants: 1.0\n",
      "  Range: 1 - 1\n",
      "  High variant count (>10): 0 pairs\n",
      "\n",
      "ðŸ“Š Right study type distribution:\n",
      "  pqtl: 69 rows\n",
      "  eqtl: 24 rows\n",
      "  tuqtl: 4 rows\n",
      "  sqtl: 3 rows\n",
      "\n",
      "ðŸ”— Variants by right study type:\n",
      "  eqtl: 1.0 avg variants (n=24.0)\n",
      "  pqtl: 1.0 avg variants (n=69.0)\n",
      "  sqtl: 1.0 avg variants (n=3.0)\n",
      "  tuqtl: 1.0 avg variants (n=4.0)\n"
     ]
    }
   ],
   "source": [
    "df_final_available_doubleJoin = analyze_gwas_colocalizations_with_available_columns_doubleJoin(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969bb423",
   "metadata": {},
   "source": [
    "Adressed comments from Mauro:\n",
    "- Affected gene mapped to the right study - ***We can get this from GWAS Study and merge on studyId to get geneID. Then,we can get the approvedSymbol from Target by merging the geneID on id***\n",
    "- Affected tissue/cell mapped to the right study - ***This can be retreived from the studyId_right***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67527734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>leftStudyLocusId</th>\n",
       "      <th>rightStudyLocusId</th>\n",
       "      <th>studyId_right</th>\n",
       "      <th>studyId_left</th>\n",
       "      <th>chromosome</th>\n",
       "      <th>h0</th>\n",
       "      <th>h1</th>\n",
       "      <th>h2</th>\n",
       "      <th>h3</th>\n",
       "      <th>h4</th>\n",
       "      <th>...</th>\n",
       "      <th>rightStudyType</th>\n",
       "      <th>studyId</th>\n",
       "      <th>studyType</th>\n",
       "      <th>traitFromSource</th>\n",
       "      <th>projectId</th>\n",
       "      <th>pubmedId</th>\n",
       "      <th>geneId</th>\n",
       "      <th>id</th>\n",
       "      <th>approvedSymbol</th>\n",
       "      <th>biotype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93cd7d9317ba066bbd0183f20d4414d6</td>\n",
       "      <td>aee53ca025534856cb3ed77e7da8e73d</td>\n",
       "      <td>UKB_PPP_EUR_MICB_Q29980_OID20593_v1</td>\n",
       "      <td>GCST90445899</td>\n",
       "      <td>17</td>\n",
       "      <td>1.878535e-89</td>\n",
       "      <td>1.990494e-33</td>\n",
       "      <td>9.437535e-60</td>\n",
       "      <td>1.878535e-97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>pqtl</td>\n",
       "      <td>UKB_PPP_EUR_MICB_Q29980_OID20593_v1</td>\n",
       "      <td>pqtl</td>\n",
       "      <td>MICB_MICA:Q29980_Q29983:OID20593:v1</td>\n",
       "      <td>UKB_PPP_EUR</td>\n",
       "      <td>None</td>\n",
       "      <td>ENSG00000204516</td>\n",
       "      <td>ENSG00000204516</td>\n",
       "      <td>MICB</td>\n",
       "      <td>protein_coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49ee6904ebd90b94a55130e956c4ee37</td>\n",
       "      <td>e7ec02bede2ef556374be1a7dc006e8b</td>\n",
       "      <td>schmiedel_2018_ge_treg_memory_ensg00000113504</td>\n",
       "      <td>GCST90479673</td>\n",
       "      <td>5</td>\n",
       "      <td>7.052834e-72</td>\n",
       "      <td>4.592746e-54</td>\n",
       "      <td>1.535647e-21</td>\n",
       "      <td>7.052834e-80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>eqtl</td>\n",
       "      <td>schmiedel_2018_ge_treg_memory_ensg00000113504</td>\n",
       "      <td>eqtl</td>\n",
       "      <td>ENSG00000113504</td>\n",
       "      <td>Schmiedel_2018</td>\n",
       "      <td>30449622</td>\n",
       "      <td>ENSG00000113504</td>\n",
       "      <td>ENSG00000113504</td>\n",
       "      <td>SLC12A7</td>\n",
       "      <td>protein_coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3ca4de9e33824bfab394b504a69e7919</td>\n",
       "      <td>994846b0cd95fd3230bedaf598a42933</td>\n",
       "      <td>fairfax_2014_microarray_monocyte_ifn24_ilmn_17...</td>\n",
       "      <td>GCST90445970</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.316137e-146</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>eqtl</td>\n",
       "      <td>fairfax_2014_microarray_monocyte_ifn24_ilmn_17...</td>\n",
       "      <td>eqtl</td>\n",
       "      <td>ILMN_1793017</td>\n",
       "      <td>Fairfax_2014</td>\n",
       "      <td>24604202</td>\n",
       "      <td>ENSG00000145214</td>\n",
       "      <td>ENSG00000145214</td>\n",
       "      <td>DGKQ</td>\n",
       "      <td>protein_coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0d050577abf1dd125314b534450c94bb</td>\n",
       "      <td>4fe83a00a4c5f8a03dfc96b94e0f3a3c</td>\n",
       "      <td>UKB_PPP_EUR_DSG3_P32926_OID21460_v1</td>\n",
       "      <td>GCST90445988</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.173614e-25</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>pqtl</td>\n",
       "      <td>UKB_PPP_EUR_DSG3_P32926_OID21460_v1</td>\n",
       "      <td>pqtl</td>\n",
       "      <td>DSG3:P32926:OID21460:v1</td>\n",
       "      <td>UKB_PPP_EUR</td>\n",
       "      <td>None</td>\n",
       "      <td>ENSG00000134757</td>\n",
       "      <td>ENSG00000134757</td>\n",
       "      <td>DSG3</td>\n",
       "      <td>protein_coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6d9920a668f3227bc8b6e65ed7518b02</td>\n",
       "      <td>20a1eb0626cd2cbb09b01702230bcaa8</td>\n",
       "      <td>UKB_PPP_EUR_LPA_P08519_OID30747_v1</td>\n",
       "      <td>GCST90319603</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.052813e-68</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>pqtl</td>\n",
       "      <td>UKB_PPP_EUR_LPA_P08519_OID30747_v1</td>\n",
       "      <td>pqtl</td>\n",
       "      <td>LPA:P08519:OID30747:v1</td>\n",
       "      <td>UKB_PPP_EUR</td>\n",
       "      <td>None</td>\n",
       "      <td>ENSG00000198670</td>\n",
       "      <td>ENSG00000198670</td>\n",
       "      <td>LPA</td>\n",
       "      <td>protein_coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>c2f0a83a5ad05b36cbee091fdbcdead4</td>\n",
       "      <td>5567e2dab9f78257c83c822e2b72d578</td>\n",
       "      <td>UKB_PPP_EUR_BPIFB1_Q8TDL5_OID20308_v1</td>\n",
       "      <td>GCST90104006</td>\n",
       "      <td>19</td>\n",
       "      <td>2.827346e-19</td>\n",
       "      <td>2.674955e-13</td>\n",
       "      <td>1.056970e-09</td>\n",
       "      <td>2.827346e-27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>pqtl</td>\n",
       "      <td>UKB_PPP_EUR_BPIFB1_Q8TDL5_OID20308_v1</td>\n",
       "      <td>pqtl</td>\n",
       "      <td>BPIFB1:Q8TDL5:OID20308:v1</td>\n",
       "      <td>UKB_PPP_EUR</td>\n",
       "      <td>None</td>\n",
       "      <td>ENSG00000125999</td>\n",
       "      <td>ENSG00000125999</td>\n",
       "      <td>BPIFB1</td>\n",
       "      <td>protein_coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>9060a44df4f55d76235954f5e68eb07b</td>\n",
       "      <td>5cfd888079f3ae146840d7296141de9f</td>\n",
       "      <td>commonmind_tx_dlpfc_naive_enst00000513430</td>\n",
       "      <td>GCST90308601</td>\n",
       "      <td>4</td>\n",
       "      <td>2.552286e-25</td>\n",
       "      <td>1.268883e-09</td>\n",
       "      <td>2.011443e-19</td>\n",
       "      <td>2.552286e-33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>eqtl</td>\n",
       "      <td>commonmind_tx_dlpfc_naive_enst00000513430</td>\n",
       "      <td>eqtl</td>\n",
       "      <td>ENST00000513430</td>\n",
       "      <td>CommonMind</td>\n",
       "      <td>31551426</td>\n",
       "      <td>ENSG00000168743</td>\n",
       "      <td>ENSG00000168743</td>\n",
       "      <td>NPNT</td>\n",
       "      <td>protein_coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>8c0bd0f7997f32c8a3d3e7ea509da426</td>\n",
       "      <td>f2317c4ff1c4ab2f774350f794d98488</td>\n",
       "      <td>UKB_PPP_EUR_TXNRD1_Q16881_OID21135_v1</td>\n",
       "      <td>GCST90475516</td>\n",
       "      <td>3</td>\n",
       "      <td>4.267389e-73</td>\n",
       "      <td>1.390133e-09</td>\n",
       "      <td>3.069770e-67</td>\n",
       "      <td>4.267389e-81</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>pqtl</td>\n",
       "      <td>UKB_PPP_EUR_TXNRD1_Q16881_OID21135_v1</td>\n",
       "      <td>pqtl</td>\n",
       "      <td>TXNRD1:Q16881:OID21135:v1</td>\n",
       "      <td>UKB_PPP_EUR</td>\n",
       "      <td>None</td>\n",
       "      <td>ENSG00000198431</td>\n",
       "      <td>ENSG00000198431</td>\n",
       "      <td>TXNRD1</td>\n",
       "      <td>protein_coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>7776c7f32478f5ec305322d64e874b1c</td>\n",
       "      <td>1d77297dcd665a712c4bdfdaaa5b0e50</td>\n",
       "      <td>UKB_PPP_EUR_CNTN1_Q12860_OID20307_v1</td>\n",
       "      <td>GCST90269596</td>\n",
       "      <td>17</td>\n",
       "      <td>1.329440e-29</td>\n",
       "      <td>7.679882e-24</td>\n",
       "      <td>1.731068e-09</td>\n",
       "      <td>1.329440e-37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>pqtl</td>\n",
       "      <td>UKB_PPP_EUR_CNTN1_Q12860_OID20307_v1</td>\n",
       "      <td>pqtl</td>\n",
       "      <td>CNTN1:Q12860:OID20307:v1</td>\n",
       "      <td>UKB_PPP_EUR</td>\n",
       "      <td>None</td>\n",
       "      <td>ENSG00000018236</td>\n",
       "      <td>ENSG00000018236</td>\n",
       "      <td>CNTN1</td>\n",
       "      <td>protein_coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ab9951e4db18ce67bb7654b15ace264f</td>\n",
       "      <td>9c079ea0bdad7737331d5b2b284c5ac3</td>\n",
       "      <td>UKB_PPP_EUR_RHOC_P08134_OID20950_v1</td>\n",
       "      <td>GCST90002360</td>\n",
       "      <td>3</td>\n",
       "      <td>2.050531e-21</td>\n",
       "      <td>1.886191e-09</td>\n",
       "      <td>1.087128e-15</td>\n",
       "      <td>2.050531e-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>pqtl</td>\n",
       "      <td>UKB_PPP_EUR_RHOC_P08134_OID20950_v1</td>\n",
       "      <td>pqtl</td>\n",
       "      <td>RHOC:P08134:OID20950:v1</td>\n",
       "      <td>UKB_PPP_EUR</td>\n",
       "      <td>None</td>\n",
       "      <td>ENSG00000155366</td>\n",
       "      <td>ENSG00000155366</td>\n",
       "      <td>RHOC</td>\n",
       "      <td>protein_coding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    leftStudyLocusId                 rightStudyLocusId  \\\n",
       "0   93cd7d9317ba066bbd0183f20d4414d6  aee53ca025534856cb3ed77e7da8e73d   \n",
       "1   49ee6904ebd90b94a55130e956c4ee37  e7ec02bede2ef556374be1a7dc006e8b   \n",
       "2   3ca4de9e33824bfab394b504a69e7919  994846b0cd95fd3230bedaf598a42933   \n",
       "3   0d050577abf1dd125314b534450c94bb  4fe83a00a4c5f8a03dfc96b94e0f3a3c   \n",
       "4   6d9920a668f3227bc8b6e65ed7518b02  20a1eb0626cd2cbb09b01702230bcaa8   \n",
       "..                               ...                               ...   \n",
       "95  c2f0a83a5ad05b36cbee091fdbcdead4  5567e2dab9f78257c83c822e2b72d578   \n",
       "96  9060a44df4f55d76235954f5e68eb07b  5cfd888079f3ae146840d7296141de9f   \n",
       "97  8c0bd0f7997f32c8a3d3e7ea509da426  f2317c4ff1c4ab2f774350f794d98488   \n",
       "98  7776c7f32478f5ec305322d64e874b1c  1d77297dcd665a712c4bdfdaaa5b0e50   \n",
       "99  ab9951e4db18ce67bb7654b15ace264f  9c079ea0bdad7737331d5b2b284c5ac3   \n",
       "\n",
       "                                        studyId_right  studyId_left  \\\n",
       "0                 UKB_PPP_EUR_MICB_Q29980_OID20593_v1  GCST90445899   \n",
       "1       schmiedel_2018_ge_treg_memory_ensg00000113504  GCST90479673   \n",
       "2   fairfax_2014_microarray_monocyte_ifn24_ilmn_17...  GCST90445970   \n",
       "3                 UKB_PPP_EUR_DSG3_P32926_OID21460_v1  GCST90445988   \n",
       "4                  UKB_PPP_EUR_LPA_P08519_OID30747_v1  GCST90319603   \n",
       "..                                                ...           ...   \n",
       "95              UKB_PPP_EUR_BPIFB1_Q8TDL5_OID20308_v1  GCST90104006   \n",
       "96          commonmind_tx_dlpfc_naive_enst00000513430  GCST90308601   \n",
       "97              UKB_PPP_EUR_TXNRD1_Q16881_OID21135_v1  GCST90475516   \n",
       "98               UKB_PPP_EUR_CNTN1_Q12860_OID20307_v1  GCST90269596   \n",
       "99                UKB_PPP_EUR_RHOC_P08134_OID20950_v1  GCST90002360   \n",
       "\n",
       "   chromosome            h0             h1            h2            h3   h4  \\\n",
       "0          17  1.878535e-89   1.990494e-33  9.437535e-60  1.878535e-97  1.0   \n",
       "1           5  7.052834e-72   4.592746e-54  1.535647e-21  7.052834e-80  1.0   \n",
       "2           4  0.000000e+00  4.316137e-146  0.000000e+00  0.000000e+00  1.0   \n",
       "3           2  0.000000e+00   5.173614e-25  0.000000e+00  0.000000e+00  1.0   \n",
       "4           6  0.000000e+00   0.000000e+00  1.052813e-68  0.000000e+00  1.0   \n",
       "..        ...           ...            ...           ...           ...  ...   \n",
       "95         19  2.827346e-19   2.674955e-13  1.056970e-09  2.827346e-27  1.0   \n",
       "96          4  2.552286e-25   1.268883e-09  2.011443e-19  2.552286e-33  1.0   \n",
       "97          3  4.267389e-73   1.390133e-09  3.069770e-67  4.267389e-81  1.0   \n",
       "98         17  1.329440e-29   7.679882e-24  1.731068e-09  1.329440e-37  1.0   \n",
       "99          3  2.050531e-21   1.886191e-09  1.087128e-15  2.050531e-29  1.0   \n",
       "\n",
       "    ... rightStudyType                                            studyId  \\\n",
       "0   ...           pqtl                UKB_PPP_EUR_MICB_Q29980_OID20593_v1   \n",
       "1   ...           eqtl      schmiedel_2018_ge_treg_memory_ensg00000113504   \n",
       "2   ...           eqtl  fairfax_2014_microarray_monocyte_ifn24_ilmn_17...   \n",
       "3   ...           pqtl                UKB_PPP_EUR_DSG3_P32926_OID21460_v1   \n",
       "4   ...           pqtl                 UKB_PPP_EUR_LPA_P08519_OID30747_v1   \n",
       "..  ...            ...                                                ...   \n",
       "95  ...           pqtl              UKB_PPP_EUR_BPIFB1_Q8TDL5_OID20308_v1   \n",
       "96  ...           eqtl          commonmind_tx_dlpfc_naive_enst00000513430   \n",
       "97  ...           pqtl              UKB_PPP_EUR_TXNRD1_Q16881_OID21135_v1   \n",
       "98  ...           pqtl               UKB_PPP_EUR_CNTN1_Q12860_OID20307_v1   \n",
       "99  ...           pqtl                UKB_PPP_EUR_RHOC_P08134_OID20950_v1   \n",
       "\n",
       "   studyType                      traitFromSource       projectId  pubmedId  \\\n",
       "0       pqtl  MICB_MICA:Q29980_Q29983:OID20593:v1     UKB_PPP_EUR      None   \n",
       "1       eqtl                      ENSG00000113504  Schmiedel_2018  30449622   \n",
       "2       eqtl                         ILMN_1793017    Fairfax_2014  24604202   \n",
       "3       pqtl              DSG3:P32926:OID21460:v1     UKB_PPP_EUR      None   \n",
       "4       pqtl               LPA:P08519:OID30747:v1     UKB_PPP_EUR      None   \n",
       "..       ...                                  ...             ...       ...   \n",
       "95      pqtl            BPIFB1:Q8TDL5:OID20308:v1     UKB_PPP_EUR      None   \n",
       "96      eqtl                      ENST00000513430      CommonMind  31551426   \n",
       "97      pqtl            TXNRD1:Q16881:OID21135:v1     UKB_PPP_EUR      None   \n",
       "98      pqtl             CNTN1:Q12860:OID20307:v1     UKB_PPP_EUR      None   \n",
       "99      pqtl              RHOC:P08134:OID20950:v1     UKB_PPP_EUR      None   \n",
       "\n",
       "             geneId               id  approvedSymbol         biotype  \n",
       "0   ENSG00000204516  ENSG00000204516            MICB  protein_coding  \n",
       "1   ENSG00000113504  ENSG00000113504         SLC12A7  protein_coding  \n",
       "2   ENSG00000145214  ENSG00000145214            DGKQ  protein_coding  \n",
       "3   ENSG00000134757  ENSG00000134757            DSG3  protein_coding  \n",
       "4   ENSG00000198670  ENSG00000198670             LPA  protein_coding  \n",
       "..              ...              ...             ...             ...  \n",
       "95  ENSG00000125999  ENSG00000125999          BPIFB1  protein_coding  \n",
       "96  ENSG00000168743  ENSG00000168743            NPNT  protein_coding  \n",
       "97  ENSG00000198431  ENSG00000198431          TXNRD1  protein_coding  \n",
       "98  ENSG00000018236  ENSG00000018236           CNTN1  protein_coding  \n",
       "99  ENSG00000155366  ENSG00000155366            RHOC  protein_coding  \n",
       "\n",
       "[100 rows x 29 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### join with target and GWAS datasets\n",
    "target = pd.read_parquet('/lustre/groups/itg/shared/referenceData/OpenTargets/temp/01_filtered_parquets/target.parquet')\n",
    "study = pd.read_parquet('/lustre/groups/itg/shared/referenceData/OpenTargets/temp/01_filtered_parquets/study.parquet') \n",
    "\n",
    "# Merge study with df_final_available_doubleJoin by projectId (study) and studyId_left (df_final_available_doubleJoin)\n",
    "coloc_study_merged = df_final_available_doubleJoin.merge(\n",
    "    study,\n",
    "    left_on='studyId_right',\n",
    "    right_on='studyId',\n",
    "    how='left',\n",
    "    suffixes=('', '_study')\n",
    ")\n",
    "\n",
    "\n",
    "coloc_study_target_merged = coloc_study_merged.merge(\n",
    "    target,\n",
    "    left_on='geneId',\n",
    "    right_on='id',\n",
    "    how='left',\n",
    "    suffixes=('', '_target')\n",
    ")\n",
    "\n",
    "# Define columns to drop (these are duplicates after merge)\n",
    "columns_to_drop = [\n",
    "    'studyId',        # duplicate of studyId_right or studyId_left\n",
    "    'id',             # duplicate of geneId\n",
    "    'approvedSymbol', # duplicate if already present\n",
    "    'biotype'         # duplicate if already present\n",
    "]\n",
    "\n",
    "coloc_study_target_merged_clean = coloc_study_target_merged.drop(columns=columns_to_drop)\n",
    "\n",
    "coloc_study_target_merged_clean = coloc_study_target_merged_clean.rename(columns={\n",
    "    'studyId_right': 'studyId',\n",
    "    'geneId': 'geneId',\n",
    "    'approvedSymbol': 'gene_symbol',\n",
    "    'biotype': 'gene_biotype'\n",
    "})\n",
    "\n",
    "coloc_study_target_merged_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3881d025",
   "metadata": {},
   "source": [
    "### Colocalization Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c9b092d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Creating dataset with available columns including numberColocalisingVariants, variantId, and isTransQtl...\n",
      "ðŸ” Checking what columns are actually available in your filtered datasets...\n",
      "  ðŸ“‹ colocalisation_coloc: ['leftStudyLocusId', 'rightStudyLocusId', 'chromosome', 'h4', 'rightStudyType', 'numberColocalisingVariants', 'h0', 'h1', 'h2', 'h3']\n",
      "  ðŸ“‹ credible_set: ['studyLocusId', 'studyId', 'variantId', 'isTransQtl']\n",
      "  ðŸ“‹ study: ['studyId', 'studyType', 'traitFromSource', 'projectId', 'pubmedId']\n",
      "\n",
      "ðŸ”§ Building query with available columns...\n",
      "  âœ… Available h-columns: ['h0', 'h1', 'h2', 'h3', 'h4']\n",
      "  âœ… Available study columns: ['traitFromSource', 'studyType']\n",
      "  âœ… numberColocalisingVariants: Available\n",
      "  âœ… variantId: Available\n",
      "  âœ… isTransQtl: Available\n",
      "  âœ… rightStudyType: Available\n",
      "\n",
      "ðŸš€ Executing query with 15 available columns...\n",
      "ðŸ“‹ Selected columns: ['leftStudyLocusId', 'rightStudyLocusId', 'studyId', 'chromosome', 'h0', 'h1', 'h2', 'h3', 'h4', 'traitFromSource', 'studyType', 'numberColocalisingVariants', 'variantId', 'isTransQtl', 'rightStudyType']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81ffc48ab51494ab7afb05741177c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset created: 100 rows\n",
      "ðŸ“‹ Final columns: ['leftStudyLocusId', 'rightStudyLocusId', 'studyId', 'chromosome', 'h0', 'h1', 'h2', 'h3', 'h4', 'traitFromSource', 'studyType', 'numberColocalisingVariants', 'variantId', 'isTransQtl', 'rightStudyType']\n",
      "\n",
      "ðŸ“Š Data completeness:\n",
      "  leftStudyLocusId: 100/100 (100.0%)\n",
      "  rightStudyLocusId: 100/100 (100.0%)\n",
      "  studyId: 100/100 (100.0%)\n",
      "  chromosome: 100/100 (100.0%)\n",
      "  h0: 100/100 (100.0%)\n",
      "  h1: 100/100 (100.0%)\n",
      "  h2: 100/100 (100.0%)\n",
      "  h3: 100/100 (100.0%)\n",
      "  h4: 100/100 (100.0%)\n",
      "  traitFromSource: 100/100 (100.0%)\n",
      "  studyType: 100/100 (100.0%)\n",
      "  numberColocalisingVariants: 100/100 (100.0%)\n",
      "  variantId: 100/100 (100.0%)\n",
      "  isTransQtl: 11/100 (11.0%)\n",
      "  rightStudyType: 100/100 (100.0%)\n",
      "\n",
      "ðŸ‘€ Sample of your dataset:\n",
      "                   leftStudyLocusId                 rightStudyLocusId  \\\n",
      "0  8c3672c9822525cc91ec0e3b164ec7ec  363d740259e5ef96863c6690bf99f42e   \n",
      "1  b27d089888c12f39398df55581513e3f  48597ac988b6a51483baee8405eb3028   \n",
      "2  e792964bda81a15aae6c989ff142e946  873e6f2a9ca732268784845917413c97   \n",
      "3  c342e99fe3535b6a7275c1fe56b0aab9  70b7b538dc50f4915586687f3938ff12   \n",
      "4  83bd609f3e6f64d82119189a2157f400  6974810b53d25e2811af09ab322045b0   \n",
      "\n",
      "                                             studyId chromosome  \\\n",
      "0                                       GCST90092966          2   \n",
      "1                                       GCST90446053         11   \n",
      "2                                       GCST90092998         19   \n",
      "3  gtex_leafcutter_pituitary_16_72057466_72058254...         16   \n",
      "4                                       GCST90092966         11   \n",
      "\n",
      "              h0             h1             h2             h3   h4  \\\n",
      "0  4.103600e-250  6.717448e-153  6.108868e-101  4.103600e-258  1.0   \n",
      "1   0.000000e+00   0.000000e+00   7.930471e-28   0.000000e+00  1.0   \n",
      "2   0.000000e+00   0.000000e+00   0.000000e+00   0.000000e+00  1.0   \n",
      "3   1.285942e-46   6.479517e-28   1.984627e-22   1.285942e-54  1.0   \n",
      "4   0.000000e+00  4.061046e-265  5.333662e-122   0.000000e+00  1.0   \n",
      "\n",
      "                                     traitFromSource studyType  \\\n",
      "0                   Triglyceride levels in small LDL      gwas   \n",
      "1  Free Cholesterol to Cholesterol in Small VLDL ...      gwas   \n",
      "2                    Free cholesterol levels in VLDL      gwas   \n",
      "3                   16:72057466:72058254:clu_26386_+      sqtl   \n",
      "4                   Triglyceride levels in small LDL      gwas   \n",
      "\n",
      "   numberColocalisingVariants         variantId  isTransQtl rightStudyType  \n",
      "0                           1    2_27508073_T_C        <NA>           gwas  \n",
      "1                           1  11_117204850_C_T        <NA>           gwas  \n",
      "2                           1   19_44730118_A_G        <NA>           gwas  \n",
      "3                           1   16_72110275_T_C       False           sqtl  \n",
      "4                           1  11_116778201_G_C        <NA>           gwas  \n",
      "\n",
      "ðŸ”— Colocalising Variants Analysis:\n",
      "  Mean variants per colocalization: 1.0\n",
      "  Median variants: 1.0\n",
      "  Range: 1 - 1\n",
      "  High variant count (>10): 0 pairs\n",
      "\n",
      "ðŸ§¬ Variant ID Analysis:\n",
      "  Unique variants: 62\n",
      "  Total rows: 100\n",
      "  Variants per row ratio: 0.62\n",
      "  Example variant IDs: ['2_27508073_T_C', '11_117204850_C_T', '19_44730118_A_G', '16_72110275_T_C', '11_116778201_G_C']\n",
      "\n",
      "ðŸ§¬ Trans-QTL Analysis:\n",
      "  Trans-QTL colocalizations: 9/100 (9.0%)\n",
      "  Cis-QTL colocalizations: 91/100 (91.0%)\n",
      "\n",
      "ðŸ§¬ Colocalization analysis:\n",
      "  Available evidence: ['h0', 'h1', 'h2', 'h3', 'h4']\n",
      "  Mean posterior probabilities:\n",
      "    h0: 0.0000\n",
      "    h1: 0.0000\n",
      "    h2: 0.0000\n",
      "    h3: 0.0000\n",
      "    h4: 1.0000\n",
      "\n",
      "ðŸ“Š Study type distribution:\n",
      "  gwas: 89 rows\n",
      "  pqtl: 9 rows\n",
      "  sqtl: 1 rows\n",
      "  tuqtl: 1 rows\n",
      "\n",
      "ðŸ“Š Right study type distribution:\n",
      "  gwas: 89 rows\n",
      "  pqtl: 9 rows\n",
      "  sqtl: 1 rows\n",
      "  tuqtl: 1 rows\n",
      "\n",
      "ðŸ”— Variants by right study type:\n",
      "  gwas: 1.0 avg variants (n=89.0)\n",
      "  pqtl: 1.0 avg variants (n=9.0)\n",
      "  sqtl: 1.0 avg variants (n=1.0)\n",
      "  tuqtl: 1.0 avg variants (n=1.0)\n",
      "\n",
      "ðŸŽ‰ Analysis complete!\n",
      "ðŸ“Š Generated 100 rows with available columns\n",
      "ðŸ’¾ Saved to: /lustre/groups/itg/shared/referenceData/OpenTargets/colocalisation_available_columns_with_variants.csv\n",
      "\n",
      "ðŸ“‹ Comparison - Requested vs Available:\n",
      "  âœ… leftStudyLocusId\n",
      "  âœ… rightStudyLocusId\n",
      "  âœ… studyId\n",
      "  âœ… traitFromSource\n",
      "  âœ… chromosome\n",
      "  âœ… studyType\n",
      "  âœ… rightStudyType\n",
      "  âœ… numberColocalisingVariants\n",
      "  âœ… variantId\n",
      "  âœ… isTransQtl\n",
      "  âœ… h0\n",
      "  âœ… h1\n",
      "  âœ… h2\n",
      "  âœ… h3\n",
      "  âœ… h4\n",
      "\n",
      "ðŸ“Š Summary:\n",
      "  âœ… Available columns: 15/15 (100.0%)\n",
      "  ðŸ“‹ Available: ['leftStudyLocusId', 'rightStudyLocusId', 'studyId', 'traitFromSource', 'chromosome', 'studyType', 'rightStudyType', 'numberColocalisingVariants', 'variantId', 'isTransQtl', 'h0', 'h1', 'h2', 'h3', 'h4']\n",
      "\n",
      "ðŸ’¡ To get missing columns:\n",
      "  â€¢ Add isTransQtl to your manifest's credible_set columns\n",
      "  â€¢ Re-run the dataset filtering process\n",
      "  â€¢ Then run this analysis to get the complete dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸš€ Creating dataset with available columns including numberColocalisingVariants, variantId, and isTransQtl...\")\n",
    "df_final_available = analyze_gwas_colocalizations_with_available_columns(output_path)\n",
    "\n",
    "if df_final_available is not None:\n",
    "    print(f\"\\nðŸŽ‰ Analysis complete!\")\n",
    "    print(f\"ðŸ“Š Generated {len(df_final_available)} rows with available columns\")\n",
    "    \n",
    "    # Save the dataset\n",
    "    available_file = main_dir / \"colocalisation_available_columns_with_variants.csv\"\n",
    "    df_final_available.to_csv(available_file, index=False)\n",
    "    print(f\"ðŸ’¾ Saved to: {available_file}\")\n",
    "    \n",
    "    # Show what you got vs what you wanted\n",
    "    print(f\"\\nðŸ“‹ Comparison - Requested vs Available:\")\n",
    "    requested = ['leftStudyLocusId','rightStudyLocusId', 'studyId', 'traitFromSource', 'chromosome', \n",
    "                'studyType', 'rightStudyType', 'numberColocalisingVariants', 'variantId',\n",
    "                'isTransQtl', 'h0', 'h1', 'h2', 'h3', 'h4']\n",
    "    \n",
    "    available_cols = []\n",
    "    missing_cols = []\n",
    "    for col in requested:\n",
    "        if col in df_final_available.columns:\n",
    "            available_cols.append(col)\n",
    "            print(f\"  âœ… {col}\")\n",
    "        else:\n",
    "            missing_cols.append(col)\n",
    "            print(f\"  âŒ {col} (not in your filtered dataset)\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Summary:\")\n",
    "    print(f\"  âœ… Available columns: {len(available_cols)}/{len(requested)} ({len(available_cols)/len(requested)*100:.1f}%)\")\n",
    "    print(f\"  ðŸ“‹ Available: {available_cols}\")\n",
    "    if missing_cols:\n",
    "        print(f\"  âŒ Missing: {missing_cols}\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ To get missing columns:\")\n",
    "    print(f\"  â€¢ Add isTransQtl to your manifest's credible_set columns\")\n",
    "    print(f\"  â€¢ Re-run the dataset filtering process\")\n",
    "    print(f\"  â€¢ Then run this analysis to get the complete dataset\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Failed to create dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a8c4db",
   "metadata": {},
   "source": [
    "### Gene-Drug-Target Association Analysis and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2aed4bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Creating gene-drug-target dataframe...\n",
      "ðŸ” Checking available datasets for gene-drug-target mapping...\n",
      "  ðŸ“‹ l2g_prediction: ['studyLocusId', 'geneId', 'score']\n",
      "  ðŸ“‹ evidence: ['studyId', 'targetId', 'diseaseId']\n",
      "  ðŸ“‹ known_drug: ['targetId', 'diseaseId', 'drugId']\n",
      "  ðŸ“‹ target: ['id', 'approvedSymbol', 'biotype']\n",
      "\n",
      "ðŸ”§ Building gene-drug-target query...\n",
      "  âœ… l2g_prediction: geneId available\n",
      "  âœ… evidence: targetId available\n",
      "  âœ… known_drug: ['drugId'] available\n",
      "  âœ… target: ['gene_symbol', 'biotype'] available\n",
      "\n",
      "ðŸš€ Executing gene-drug-target query...\n",
      "ðŸ“‹ Selected columns: ['geneId', 'studyLocusId', 'l2g_score', 'targetId', 'diseaseId', 'drugId', 'gene_symbol', 'biotype']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a0476a52634e659908f4178c4b6ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset created: 1000 rows\n",
      "ðŸ“‹ Final columns: ['geneId', 'studyLocusId', 'l2g_score', 'targetId', 'diseaseId', 'drugId', 'gene_symbol', 'biotype']\n",
      "\n",
      "ðŸ“Š Gene-Drug-Target Summary:\n",
      "  Unique genes: 1\n",
      "  Unique drugs: 6\n",
      "  Unique targets: 1\n",
      "\n",
      "ðŸŽ¯ Locus-to-Gene Score Distribution:\n",
      "  Mean score: 1.000\n",
      "  High confidence (>0.8): 1000 genes\n",
      "\n",
      "ðŸ‘€ Sample gene-drug-target associations:\n",
      "            geneId gene_symbol         drugId         targetId  l2g_score\n",
      "0  ENSG00000113578        FGF1  CHEMBL4298055  ENSG00000113578   0.999998\n",
      "1  ENSG00000113578        FGF1  CHEMBL4298055  ENSG00000113578   0.999998\n",
      "2  ENSG00000113578        FGF1   CHEMBL413376  ENSG00000113578   0.999998\n",
      "3  ENSG00000113578        FGF1   CHEMBL265502  ENSG00000113578   0.999998\n",
      "4  ENSG00000113578        FGF1   CHEMBL265502  ENSG00000113578   0.999998\n",
      "\n",
      "ðŸŽ‰ Analysis complete!\n",
      "ðŸ“Š Generated 1000 gene-drug-target associations\n",
      "ðŸ’¾ Saved to: /lustre/groups/itg/shared/referenceData/OpenTargets/gene_drug_target_associations.csv\n",
      "\n",
      "ðŸ”¬ Key Insights:\n",
      "  Top genes by drug count:\n",
      "    FGF1: 6 drugs\n",
      "\n",
      "ðŸ“Š Gene-Drug-Target Data Analysis:\n",
      "  Total associations: 1,000\n",
      "  Unique genes: 1\n",
      "  Unique drugs: 6\n",
      "  Unique targets: 1\n",
      "  Unique diseases: 30\n",
      "\n",
      "ðŸ” Top genes by drug count:\n",
      "    FGF1: 6 drugs\n",
      "\n",
      "ðŸŽ¯ L2G Score Statistics:\n",
      "count    1.000000e+03\n",
      "mean     9.999979e-01\n",
      "std      4.443114e-16\n",
      "min      9.999979e-01\n",
      "25%      9.999979e-01\n",
      "50%      9.999979e-01\n",
      "75%      9.999979e-01\n",
      "max      9.999979e-01\n",
      "Name: l2g_score, dtype: float64\n",
      "\n",
      "ðŸ§¬ Biotype Distribution:\n",
      "biotype\n",
      "protein_coding    1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ‘€ Sample associations:\n",
      "            geneId                      studyLocusId  l2g_score  \\\n",
      "0  ENSG00000113578  4932d76f08996772aafd8743a1b34984   0.999998   \n",
      "1  ENSG00000113578  4932d76f08996772aafd8743a1b34984   0.999998   \n",
      "2  ENSG00000113578  4932d76f08996772aafd8743a1b34984   0.999998   \n",
      "3  ENSG00000113578  4932d76f08996772aafd8743a1b34984   0.999998   \n",
      "4  ENSG00000113578  4932d76f08996772aafd8743a1b34984   0.999998   \n",
      "\n",
      "          targetId      diseaseId         drugId gene_symbol         biotype  \n",
      "0  ENSG00000113578    EFO_0001421  CHEMBL4298055        FGF1  protein_coding  \n",
      "1  ENSG00000113578    EFO_0000400  CHEMBL4298055        FGF1  protein_coding  \n",
      "2  ENSG00000113578    EFO_0003060   CHEMBL413376        FGF1  protein_coding  \n",
      "3  ENSG00000113578  MONDO_0008315   CHEMBL265502        FGF1  protein_coding  \n",
      "4  ENSG00000113578    EFO_0003060   CHEMBL265502        FGF1  protein_coding  \n"
     ]
    }
   ],
   "source": [
    "df_gene_target = run_gene_drug_analysis()\n",
    "\n",
    "if df_gene_target is not None:\n",
    "    print(\"\\nðŸ“Š Gene-Drug-Target Data Analysis:\")\n",
    "    print(f\"  Total associations: {len(df_gene_target):,}\")\n",
    "    print(f\"  Unique genes: {df_gene_target['geneId'].nunique():,}\")\n",
    "    print(f\"  Unique drugs: {df_gene_target['drugId'].nunique():,}\")\n",
    "    print(f\"  Unique targets: {df_gene_target['targetId'].nunique():,}\")\n",
    "    print(f\"  Unique diseases: {df_gene_target['diseaseId'].nunique():,}\")\n",
    "\n",
    "    # Top genes by drug count\n",
    "    top_genes = df_gene_target.groupby('gene_symbol')['drugId'].nunique().sort_values(ascending=False).head(5)\n",
    "    print(\"\\nðŸ” Top genes by drug count:\")\n",
    "    for gene, count in top_genes.items():\n",
    "        print(f\"    {gene}: {count} drugs\")\n",
    "\n",
    "    # L2G score distribution\n",
    "    print(\"\\nðŸŽ¯ L2G Score Statistics:\")\n",
    "    print(df_gene_target['l2g_score'].describe())\n",
    "\n",
    "    # Drug development phase analysis (if available)\n",
    "    if 'phase' in df_gene_target.columns:\n",
    "        print(\"\\nðŸ’Š Drug Development Phases:\")\n",
    "        print(df_gene_target['phase'].value_counts().sort_index())\n",
    "\n",
    "    # Biotype distribution\n",
    "    print(\"\\nðŸ§¬ Biotype Distribution:\")\n",
    "    print(df_gene_target['biotype'].value_counts())\n",
    "\n",
    "    # Show sample data\n",
    "    print(\"\\nðŸ‘€ Sample associations:\")\n",
    "    print(df_gene_target.head())\n",
    "else:\n",
    "    print(\"âŒ No gene-drug-target associations found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
